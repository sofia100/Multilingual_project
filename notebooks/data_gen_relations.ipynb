{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /assets/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the model you need to use. Use the above to see the list of models.\n",
    "model_name_or_path = \"/assets/models/mistralai-mistral-instruct-7b-v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install protobuf\n",
    "# %pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# LLaMa's tokenizer does not have a valid PAD token, so we need to initialize this as so\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# For decoder-only models, just to be safe, also do:\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eed983de20465caf71d9e19dfb2e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # By default, map different parts of the model to available GPU(s).\n",
    "    device_map=\"auto\",\n",
    "    # Loading the model in full precision can use a lot of\n",
    "    # of memory, so we quantize it using reduced precision types.\n",
    "    torch_dtype='bfloat16'\n",
    ")\n",
    "\n",
    "# Best practices\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on using the transformers library and its components can be found here: https://huggingface.co/docs/transformers/llm_tutorial\n",
    "\n",
    "Specifically, for text generation, the following can be useful:\n",
    "- https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
    "- https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_PROMPT = '''Generate 10 English sentences each for the relations: \n",
    "grandmother, grandfather, uncle, aunt, brother-in-law, sister-in-law, cousin, nephew, niece. \n",
    "For each relation, generate 10 sentences using the following topics: games, deep talks, questions, exclamations, and other forms of speeches. \n",
    "Ensure the sentences include different forms of possessive pronouns (e.g., my, their, his, her) for each of the 9 relations. Avoid using short/colloquial terms for the relations.\n",
    "Only provide the response as a Python list of strings for all categories. No need to segregate them in any manner.\n",
    "Sample output: \n",
    "<<<[\n",
    "\"My grandmother and I play chess together.\", \n",
    "...\n",
    "\"Their grandfather and I have deep talks.\", \n",
    "...\n",
    "\"Due to the heartache, my brother-in-law was not present in the function.\",\n",
    "...\n",
    "\"For the party, my uncle will pick and drop you.\",\n",
    "...\n",
    "\"I have the best sister-in-law in the world!\",\n",
    "...\n",
    "\"I am very grateful to my aunt to take care of my mother when I was not available\",\n",
    "...\n",
    "... \n",
    "] (total 90 sentences: for each relation -> 10 sentences)>>>'''\n",
    "\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs.to(model.device),\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=2,\n",
    "            num_beams=5,\n",
    "            max_new_tokens=2000\n",
    "        )\n",
    "\n",
    "        outputs = tokenizer.batch_decode(\n",
    "            outputs, skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "    return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for output in outputs:\n",
    "#     print(\"o/p::\", output)\n",
    "    # print('-' * 50)\n",
    "    # print()\n",
    "# save the outputs to a file named : generated_outputs.txt\n",
    "# open in append mode\n",
    "\n",
    "def save_outputs(outputs):\n",
    "    with open('generated_outputs.txt', 'a') as f:\n",
    "        for output in outputs:\n",
    "            f.write(output + '\\n')\n",
    "            # print(output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(outputs):\n",
    "    response = outputs[0]\n",
    "    sentences = response[response.find(TASK_PROMPT) + len(TASK_PROMPT):]\n",
    "    sentences = sentences[sentences.find('[') + 1:sentences.find(']')]  \n",
    "    sentences = sentences.split(',\\n')\n",
    "    sentences = [sentence.strip() for sentence in sentences]\n",
    "    sentences = [sentence[1:-1] for sentence in sentences]\n",
    "    # print(sentences, type(sentences))\n",
    "    return sentences\n",
    "\n",
    "# extract_sentences(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentences to a file named : generated_sentences.txt\n",
    "def save_sentences(sentences):\n",
    "    with open('generated_sentences.txt', 'a+') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt template:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts=[]\n",
    "# techniques = examples.keys()\n",
    "topics=[\"occupation\", \" religion\", \" sport\", \" politics\", \"  health\", \" finance\", \" education\", \" farming\", \" entertainment\", \"  news\", \"  daily conversation\", \"  weather\", \" technology\", \" conflicts\", \" controversials\", \n",
    "        \" international\", \"  UN\", \" travel\", \" tourism\", \" shopping\", \" baby care\", \" valentines\", \"  soldiers\", \" prisioners\", \" soul actions\", \" nature\", \" pollution\", \" bio hazards\", \" elders\", \" family\", \n",
    "        \"  social studies\", \" maths\", \" literature\", \"  physics\", \" chemistry\", \"  biology\", \" Indian history \", \" civics\", \" geography\", \"  computer\", \" physical education\", \"  arts and craft\", \"  food\", \n",
    "        \" clothes\", \" water shortage\", \" road blockage\", \" traffic\", \" arriving late\", \" bargaining\", \" toys\", \" games\", \" deep talks\", \"declarative\", \"interrogative\", \"imperative\",  \"exclamatory\", \"safe work space\", \n",
    "        \"medical checkups\", \"self-reliant\", \"future India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = [ \"legal\", \"governance\",  \n",
    "  \"STEM\" ,   \"business\", \"sports\",  \"culture\", \"Alarm\",\n",
    "    \"Audio\",\n",
    "    \"Calendar / Events\",  # Added space for better readability\n",
    "    \"Communication\",\n",
    "    \"DateTime\",\n",
    "    \"Email\",\n",
    "    \"Finance\",  # Likely, but limited information available\n",
    "    \"General\",\n",
    "    \"Home Automation\",  # Likely, but limited information available\n",
    "    \"Location\",\n",
    "    \"Music\",\n",
    "    \"News\",\n",
    "    \"Reminders\",\n",
    "    \"Social Media\",  # Likely, but limited information available\n",
    "    \"Travel\",\n",
    "    \"Weather\",\n",
    "    \"BookRestaurant\",\n",
    "    \"BookFlight\",\n",
    "    \"BookHotel\",\n",
    "    \"GetDirections\",\n",
    "    \"GetPlaceDetails\",\n",
    "    \"GetWeather\",\n",
    "    \"SearchForInformation\",\n",
    "    \"GetNews\",\n",
    "    \"PlayMusic\",\n",
    "    \"ControlVolume\",\n",
    "    \"SetAlarm\",\n",
    "    \"CancelAlarm\",\n",
    "    \"SetReminder\",\n",
    "    \"MakeCall\",\n",
    "    \"SendMessage\",\n",
    "    \"SendEmail\",\n",
    "    \"CheckEmail\",\n",
    "    \"GetTime\",\n",
    "    \"GetDate\",\n",
    "    \"Greeting\",\n",
    "    \"Farewell\",\n",
    "    \"AskHowAreYou\",\n",
    "    \"ThankYou\",\n",
    "    \"TellAJoke\",\n",
    "    \"TurnOnLights\",  # and other home automation actions\n",
    "    \"Social media interactions\",  # posting, liking, etc.\n",
    "    \"Financial transactions\"  ]\n",
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  legal 0\n",
      "sentences extracted for topic: legal 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  governance 1\n",
      "sentences extracted for topic: governance 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  STEM 2\n",
      "sentences extracted for topic: STEM 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  business 3\n",
      "sentences extracted for topic: business 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  sports 4\n",
      "sentences extracted for topic: sports 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  culture 5\n",
      "sentences extracted for topic: culture 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Alarm 6\n",
      "sentences extracted for topic: Alarm 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Audio 7\n",
      "sentences extracted for topic: Audio 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Calendar / Events 8\n",
      "sentences extracted for topic: Calendar / Events 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Communication 9\n",
      "sentences extracted for topic: Communication 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  DateTime 10\n",
      "sentences extracted for topic: DateTime 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Email 11\n",
      "sentences extracted for topic: Email 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Finance 12\n",
      "sentences extracted for topic: Finance 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  General 13\n",
      "sentences extracted for topic: General 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Home Automation 14\n",
      "sentences extracted for topic: Home Automation 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Location 15\n",
      "sentences extracted for topic: Location 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Music 16\n",
      "sentences extracted for topic: Music 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  News 17\n",
      "sentences extracted for topic: News 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Reminders 18\n",
      "sentences extracted for topic: Reminders 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Social Media 19\n",
      "sentences extracted for topic: Social Media 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Travel 20\n",
      "sentences extracted for topic: Travel 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Weather 21\n",
      "sentences extracted for topic: Weather 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  BookRestaurant 22\n",
      "sentences extracted for topic: BookRestaurant 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  BookFlight 23\n",
      "sentences extracted for topic: BookFlight 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  BookHotel 24\n",
      "sentences extracted for topic: BookHotel 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetDirections 25\n",
      "sentences extracted for topic: GetDirections 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetPlaceDetails 26\n",
      "sentences extracted for topic: GetPlaceDetails 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetWeather 27\n",
      "sentences extracted for topic: GetWeather 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  SearchForInformation 28\n",
      "sentences extracted for topic: SearchForInformation 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetNews 29\n",
      "sentences extracted for topic: GetNews 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  PlayMusic 30\n",
      "sentences extracted for topic: PlayMusic 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  ControlVolume 31\n",
      "sentences extracted for topic: ControlVolume 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  SetAlarm 32\n",
      "sentences extracted for topic: SetAlarm 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  CancelAlarm 33\n",
      "sentences extracted for topic: CancelAlarm 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  SetReminder 34\n",
      "sentences extracted for topic: SetReminder 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  MakeCall 35\n",
      "sentences extracted for topic: MakeCall 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  SendMessage 36\n",
      "sentences extracted for topic: SendMessage 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  SendEmail 37\n",
      "sentences extracted for topic: SendEmail 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  CheckEmail 38\n",
      "sentences extracted for topic: CheckEmail 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetTime 39\n",
      "sentences extracted for topic: GetTime 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  GetDate 40\n",
      "sentences extracted for topic: GetDate 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Greeting 41\n",
      "sentences extracted for topic: Greeting 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Farewell 42\n",
      "sentences extracted for topic: Farewell 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  AskHowAreYou 43\n",
      "sentences extracted for topic: AskHowAreYou 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  ThankYou 44\n",
      "sentences extracted for topic: ThankYou 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  TellAJoke 45\n",
      "sentences extracted for topic: TellAJoke 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  TurnOnLights 46\n",
      "sentences extracted for topic: TurnOnLights 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs done for topic:  Social media interactions 47\n",
      "sentences extracted for topic: Social media interactions 47\n",
      "outputs done for topic:  Financial transactions 48\n",
      "sentences extracted for topic: Financial transactions 48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for technique in techniques:\n",
    "    # pick every 10 topics from topics in a loop\n",
    "for i in range(0, len(topics)):\n",
    "    # print(topics[i:i+10])\n",
    "    # print(\"Generate 100 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "    # prompts.append(\"Generate 10 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "    prompt = '''Generate 10 English sentences each for the relations: \n",
    "grandmother, grandfather, uncle, aunt, brother-in-law, sister-in-law, cousin, nephew, niece. \n",
    "For each relation, generate 10 sentences using the following topic: ''' + topics[i]+''' \n",
    "Ensure the sentences include different forms of possessive pronouns (e.g., my, their, his, her) for each of the 9 relations. Avoid using short/colloquial terms for the relations.\n",
    "Only provide the response as a Python list of strings for all categories. No need to segregate them in any manner.\n",
    "Sample output: \n",
    "<<<[\n",
    "\"My grandmother and I play chess together.\", \n",
    "...\n",
    "\"Their grandfather and I have deep talks.\", \n",
    "...\n",
    "\"Due to the heartache, my brother-in-law was not present in the function.\",\n",
    "...\n",
    "\"For the party, my uncle will pick and drop you.\",\n",
    "...\n",
    "\"I have the best sister-in-law in the world!\",\n",
    "...\n",
    "\"I am very grateful to my aunt to take care of my mother when I was not available\",\n",
    "...\n",
    "... \n",
    "] (total 90 sentences: for each relation -> 10 sentences)>>>'''  \n",
    "    prompts.append(prompt)\n",
    "\n",
    "    outputs= generate_response(prompt=prompt)\n",
    "    print(\"outputs done for topic: \", topics[i], i)\n",
    "    save_outputs(outputs)\n",
    "    sentences= extract_sentences(outputs)\n",
    "    print(\"sentences extracted for topic:\", topics[i], i)\n",
    "    save_sentences(sentences)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RLHF'd models (LLaMa-3.1 Instruct, etc.), an additional prompt formatting step is needed to ensure that the model is able to generate the desired output. The template is applied using `tokenizer.apply_chat_template` function, and basically adds formatting tokens to your prompt. Use it only with instruction-fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %pip install jinja2>=3.1.0\n",
    "\n",
    "# TASK_PROMPT = \"Please answer my question. What is the capital of India?\"\n",
    "# TASK_CONVERSATION = [\n",
    "#     # System Prompt: This is optional, and not all models support this.\n",
    "#     # But use it if you need explicit instructions to be followed.\n",
    "#     dict(role='system', content='You are a helpful assistant.'),\n",
    "#     # Your message (as if on the web interface) goes here.\n",
    "#     # Past history can be added to this conversation too.\n",
    "#     dict(role='user', content=TASK_PROMPT)\n",
    "# ]\n",
    "\n",
    "# # Format the conversation to a text prompt, using apply chat template.\n",
    "# conversation_prompt = tokenizer.apply_chat_template(\n",
    "#     TASK_CONVERSATION,\n",
    "#     tokenize=False,\n",
    "#     # Needed to allow the model to start its reply instead of completing yours.\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# # We skip special tokens because the template already adds them. This is an overlooked thing, so be careful.\n",
    "# inputs = tokenizer(conversation_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# # Generation process is the same as before.\n",
    "# with torch.inference_mode():\n",
    "#     outputs = model.generate(\n",
    "#         **inputs.to(model.device),\n",
    "#         temperature=0.2,\n",
    "#         do_sample=True,\n",
    "#         num_return_sequences=2,\n",
    "#         num_beams=2,\n",
    "#         max_new_tokens=10\n",
    "#     )\n",
    "\n",
    "#     outputs = tokenizer.batch_decode(\n",
    "#         outputs, skip_special_tokens=True,\n",
    "#         clean_up_tokenization_spaces=True\n",
    "#     )\n",
    "\n",
    "#     for output in outputs:\n",
    "#         print(output)\n",
    "#         print('-' * 50)\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
