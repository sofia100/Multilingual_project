{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /assets/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the model you need to use. Use the above to see the list of models.\n",
    "model_name_or_path = \"/assets/models/meta-llama-3.2-instruct-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "print(\"tokenizer loaded\")\n",
    "\n",
    "# LLaMa's tokenizer does not have a valid PAD token, so we need to initialize this as so\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# For decoder-only models, just to be safe, also do:\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b44bbac9de8648349c1bdd9c20d8e01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    # By default, map different parts of the model to available GPU(s).\n",
    "    device_map=\"auto\",\n",
    "    # Loading the model in full precision can use a lot of\n",
    "    # of memory, so we quantize it using reduced precision types.\n",
    "    torch_dtype='bfloat16'\n",
    ")\n",
    "\n",
    "# Best practices\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information on using the transformers library and its components can be found here: https://huggingface.co/docs/transformers/llm_tutorial\n",
    "\n",
    "Specifically, for text generation, the following can be useful:\n",
    "- https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
    "- https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "TASK_PROMPT = '''Generate 10 English sentences each for the relations: \n",
    "grandmother, grandfather, uncle, aunt, brother-in-law, sister-in-law, cousin, nephew, niece. \n",
    "For each relation, generate 10 sentences using the following topics: games, deep talks, questions, exclamations, and other forms of speeches. \n",
    "Ensure the sentences are varied topics and include different forms of possessive pronouns (e.g., my, their, his, her). \n",
    "Only provide the response as a Python list of strings. \n",
    "Sample output: \n",
    "[\"My grandmother and I play chess together.\", \"Their grandmother and I have deep talks.\", ... ]'''\n",
    "\n",
    "inputs = tokenizer(TASK_PROMPT, return_tensors=\"pt\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2,\n",
    "        num_beams=5,\n",
    "        max_new_tokens=1000\n",
    "    )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(\n",
    "        outputs, skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 10 English sentences each for the relations: \n",
      "grandmother, grandfather, uncle, aunt, brother-in-law, sister-in-law, cousin, nephew, niece. \n",
      "For each relation, generate 10 sentences using the following topics: games, deep talks, questions, exclamations, and other forms of speeches. \n",
      "Ensure the sentences are varied topics and include different forms of possessive pronouns (e.g., my, their, his, her). \n",
      "Only provide the response as a Python list of strings. \n",
      "Sample output: \n",
      "[\"My grandmother and I play chess together.\", \"Their grandmother and I have deep talks.\",... ] \n",
      "\n",
      "```python\n",
      "def generate_sentences():\n",
      "    relations = [\"grandmother\", \"grandfather\", \"uncle\", \"aunt\", \"brother-in-law\", \"sister-in-law\", \"cousin\", \"nephew\", \"niece\"]\n",
      "    topics = [\"games\", \"deep talks\", \"questions\", \"exclamations\", \"other forms of speeches\"]\n",
      "    sentences = []\n",
      "\n",
      "    for relation in relations:\n",
      "        for topic in topics:\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I\n",
      "Generate 10 English sentences each for the relations: \n",
      "grandmother, grandfather, uncle, aunt, brother-in-law, sister-in-law, cousin, nephew, niece. \n",
      "For each relation, generate 10 sentences using the following topics: games, deep talks, questions, exclamations, and other forms of speeches. \n",
      "Ensure the sentences are varied topics and include different forms of possessive pronouns (e.g., my, their, his, her). \n",
      "Only provide the response as a Python list of strings. \n",
      "Sample output: \n",
      "[\"My grandmother and I play chess together.\", \"Their grandmother and I have deep talks.\",... ] \n",
      "\n",
      "```python\n",
      "def generate_sentences():\n",
      "    relations = [\"grandmother\", \"grandfather\", \"uncle\", \"aunt\", \"brother-in-law\", \"sister-in-law\", \"cousin\", \"nephew\", \"niece\"]\n",
      "    topics = [\"games\", \"deep talks\", \"questions\", \"exclamations\", \"other forms of speeches\"]\n",
      "    sentences = []\n",
      "\n",
      "    for relation in relations:\n",
      "        for topic in topics:\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "            sentences.append(f\"{relation} and I {topic}.\")\n",
      "    return sentences\n",
      "\n",
      "print(generate_sentences())\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for output in outputs:\n",
    "#     print(\"o/p::\", output)\n",
    "    # print('-' * 50)\n",
    "    # print()\n",
    "# save the outputs to a file named : generated_outputs.txt\n",
    "# open in append mode\n",
    "with open('generated_outputs.txt', 'a') as f:\n",
    "    for output in outputs:\n",
    "        f.write(output + '\\n')\n",
    "        print(output)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'nephew', 'niece']\n"
     ]
    }
   ],
   "source": [
    "response = outputs[0]\n",
    "sentences = response[response.find(TASK_PROMPT) + len(TASK_PROMPT):]\n",
    "sentences = sentences[sentences.find('[') + 1:sentences.find(']')]  \n",
    "sentences = sentences.split(',')\n",
    "sentences = [sentence.strip() for sentence in sentences]\n",
    "sentences = [sentence[1:-1] for sentence in sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the sentences to a file named : generated_sentences.txt\n",
    "with open('generated_sentences.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RLHF'd models (LLaMa-3.1 Instruct, etc.), an additional prompt formatting step is needed to ensure that the model is able to generate the desired output. The template is applied using `tokenizer.apply_chat_template` function, and basically adds formatting tokens to your prompt. Use it only with instruction-fine-tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 2.11.3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m      4\u001b[0m TASK_CONVERSATION \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# System Prompt: This is optional, and not all models support this.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# But use it if you need explicit instructions to be followed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mdict\u001b[39m(role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, content\u001b[38;5;241m=\u001b[39mTASK_PROMPT)\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Format the conversation to a text prompt, using apply chat template.\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m conversation_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTASK_CONVERSATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Needed to allow the model to start its reply instead of completing yours.\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# We skip special tokens because the template already adds them. This is an overlooked thing, so be careful.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(conversation_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1794\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m     )\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[0;32m-> 1794\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_jinja_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(conversation[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1798\u001b[0m ):\n\u001b[1;32m   1799\u001b[0m     conversations \u001b[38;5;241m=\u001b[39m conversation\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1920\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._compile_jinja_template\u001b[0;34m(self, chat_template)\u001b[0m\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_chat_template requires jinja2 to be installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(jinja2\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_chat_template requires jinja2>=3.1.0 to be installed. Your version is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjinja2\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1922\u001b[0m     )\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[1;32m   1925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TemplateError(message)\n",
      "\u001b[0;31mImportError\u001b[0m: apply_chat_template requires jinja2>=3.1.0 to be installed. Your version is 2.11.3."
     ]
    }
   ],
   "source": [
    "# %pip install jinja2>=3.1.0\n",
    "\n",
    "TASK_PROMPT = \"Please answer my question. What is the capital of India?\"\n",
    "TASK_CONVERSATION = [\n",
    "    # System Prompt: This is optional, and not all models support this.\n",
    "    # But use it if you need explicit instructions to be followed.\n",
    "    dict(role='system', content='You are a helpful assistant.'),\n",
    "    # Your message (as if on the web interface) goes here.\n",
    "    # Past history can be added to this conversation too.\n",
    "    dict(role='user', content=TASK_PROMPT)\n",
    "]\n",
    "\n",
    "# Format the conversation to a text prompt, using apply chat template.\n",
    "conversation_prompt = tokenizer.apply_chat_template(\n",
    "    TASK_CONVERSATION,\n",
    "    tokenize=False,\n",
    "    # Needed to allow the model to start its reply instead of completing yours.\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "# We skip special tokens because the template already adds them. This is an overlooked thing, so be careful.\n",
    "inputs = tokenizer(conversation_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "# Generation process is the same as before.\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        temperature=0.2,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=2,\n",
    "        num_beams=2,\n",
    "        max_new_tokens=10\n",
    "    )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(\n",
    "        outputs, skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    for output in outputs:\n",
    "        print(output)\n",
    "        print('-' * 50)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
