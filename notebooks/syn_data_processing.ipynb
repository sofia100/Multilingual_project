{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of the notebook:\n",
    "This notebook will pre-process the texts generated by Chatgpt 4o /Gemini-3 and make a dictionary:\n",
    "\n",
    "- root_word_1:\n",
    "    - language_1:\n",
    "        - indic_relation_1: [sentence_1, sentence_2, ...]\n",
    "        - indic_relation_2: [sentence_1, sentence_2, ...]\n",
    "    - language_2:\n",
    "        - indic_relation_1: [sentence_1, sentence_2, ...]\n",
    "        - indic_relation_2: [sentence_1, sentence_2, ...]\n",
    "...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES= 6\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/home/sofia/cache_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(female_data_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'Hindi'\n",
    "examples= {\n",
    "    \"Subject Pronouns & Gender-Specific Endings\": {\n",
    "        0: [\"वह काम करता है।\", \"वह दौड़ लगाता है।\", \"वह पढ़ाई करता है।\", \"वह खाना पकाता है।\", \"वह गाना गाता है।\"],\n",
    "        1: [\"वह काम करती है।\", \"वह दौड़ लगाती है।\", \"वह पढ़ाई करती है।\", \"वह खाना पकाती है।\", \"वह गाना गाती है।\"]\n",
    "    },\n",
    "    \"Verb Endings in Past Tense\": {\n",
    "        0: [\"उसने किताब पढ़ा।\", \"उसने चाय बनाया।\", \"उसने काम खत्म किया।\", \"उसने दौड़ पूरा किया।\", \"उसने गाना गाया।\"],\n",
    "        1: [\"उसने किताब पढ़ी।\", \"उसने चाय बनाई।\", \"उसने काम खत्म किया।\", \"उसने दौड़ पूरी की।\", \"उसने गाना गाया।\"]\n",
    "    },\n",
    "    \"Future Tense Differentiation\": {\n",
    "        0: [\"वह कल स्कूल जाएगा।\", \"वह ऑफिस समय पर पहुंचेगा।\", \"वह नई जगह घूमने जाएगा।\", \"वह परीक्षा में अच्छा करेगा।\", \"वह अपने माता-पिता से मिलेगा।\"],\n",
    "        1: [\"वह कल स्कूल जाएगी।\", \"वह ऑफिस समय पर पहुंचेगी।\", \"वह नई जगह घूमने जाएगी।\", \"वह परीक्षा में अच्छा करेगी।\", \"वह अपने माता-पिता से मिलेगी।\"]\n",
    "    },\n",
    "    \"Plural Gender-Specific Forms\": {\n",
    "        0: [\"वे बाजार गए।\", \"वे पार्क में खेले।\", \"वे गाँव चले गए।\", \"वे फिल्म देखने गए।\", \"वे पार्टी में गए।\"],\n",
    "        1: [\"वे बाजार गईं।\", \"वे पार्क में खेलीं।\", \"वे गाँव चली गईं।\", \"वे फिल्म देखने गईं।\", \"वे पार्टी में गईं।\"]\n",
    "    },\n",
    "    \"Adjective Differentiation\": {\n",
    "        0: [\"वह बहुत खुश है।\", \"वह बहुत थका हुआ है।\", \"वह ऊर्जावान महसूस कर रहा है।\", \"वह बीमार लग रहा है।\", \"वह परेशान दिख रहा है।\"],\n",
    "        1: [\"वह बहुत खुशी है।\", \"वह बहुत थकी हुई है।\", \"वह ऊर्जावान महसूस कर रही है।\", \"वह बीमार लग रही है।\", \"वह परेशान दिख रही है।\"]\n",
    "    },\n",
    "    \"Pronoun & Possessive Pronoun Differentiation\": {\n",
    "        0: [\"उसका घर बहुत बड़ा है।\", \"उसका दोस्त बहुत अच्छा है।\", \"उसका स्कूल पास में है।\", \"उसका सपना सच हुआ।\", \"उसका मोबाइल खो गया।\"],\n",
    "        1: [\"उसकी किताब बहुत रोचक है।\", \"उसकी सहेली बहुत अच्छी है।\", \"उसकी दुकान बहुत प्रसिद्ध है।\", \"उसकी आवाज़ बहुत मीठी है।\", \"उसकी गाड़ी नई है।\"]\n",
    "    },\n",
    "    \"Name-Based Differentiation\": {\n",
    "        0: [\"रवि स्कूल गया।\", \"अमित ऑफिस चला गया।\", \"सौरभ क्रिकेट खेल रहा है।\", \"आदित्य खाना बना रहा है।\", \"मनोज दिल्ली में रहता है।\"],\n",
    "        1: [\"सीमा स्कूल गई।\", \"निता ऑफिस चली गई।\", \"प्रियंका क्रिकेट खेल रही है।\", \"सुषमा खाना बना रही है।\", \"राधा दिल्ली में रहती है।\"]\n",
    "    },\n",
    "    \"Object-Verb Agreement\": {\n",
    "        0: [\"वह खाना खा चुका है।\", \"वह यात्रा कर चुका है।\", \"वह परीक्षा पास कर चुका है।\", \"वह अपनी रिपोर्ट जमा कर चुका है।\", \"वह घर पहुंच चुका है।\"],\n",
    "        1: [\"वह खाना खा चुकी है।\", \"वह यात्रा कर चुकी है।\", \"वह परीक्षा पास कर चुकी है।\", \"वह अपनी रिपोर्ट जमा कर चुकी है।\", \"वह घर पहुंच चुकी है।\"]\n",
    "    },\n",
    "    \"Sentence Ending Words\": {\n",
    "        0: [\"आप कैसे हैं?\", \"तुम कहाँ जा रहे हो?\", \"क्या तुमने अपना काम खत्म कर लिया?\", \"क्या तुम खुश हो?\", \"क्या तुम्हें मेरी बात समझ आई?\"],\n",
    "        1: [\"आप कैसी हैं?\", \"तुम कहाँ जा रही हो?\", \"क्या तुमने अपना काम खत्म कर लिया?\", \"क्या तुम खुश हो?\", \"क्या तुम्हें मेरी बात समझ आई?\"]\n",
    "    }\n",
    "}\n",
    "prompts=[]\n",
    "techniques = examples.keys()\n",
    "topics=[\"occupation\", \" religion\", \" sport\", \" politics\", \"  health\", \" finance\", \" education\", \" farming\",\n",
    "         \" entertainment\", \"  news\", \"  daily conversation\", \"  weather\", \" technology\", \" conflicts\",\n",
    "           \" controversials\", \" international\", \"  UN\", \" travel\", \" tourism\", \" shopping\", \" baby care\", \n",
    "           \" valentines\", \"  soldiers\", \" prisioners\", \" soul actions\", \" nature\", \" pollution\", \n",
    "           \" bio hazards\", \" elders\", \" family\", \"  social studies\", \" maths\", \" literature\", \"  physics\", \n",
    "           \" chemistry\", \"  biology\", \" Indian history \", \" civics\", \" geography\", \"  computer\", \n",
    "           \" physical education\", \"  arts and craft\", \"  food\", \" clothes\", \" water shortage\", \" road blockage\",\n",
    "             \" traffic\", \" arriving late\", \" bargaining\", \" toys\", \" games\", \" deep talks\", \"declarative\",\n",
    "               \"interrogative\", \"imperative\",  \"exclamatory\", \"safe work space\", \"medical checkups\", \n",
    "               \"self-reliant\", \"future India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for technique in techniques:\n",
    "    # pick every 10 topics from topics in a loop\n",
    "    for i in range(0, len(topics), 10):\n",
    "        # print(topics[i:i+10])\n",
    "        # print(\"Generate 100 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        prompts.append(\"Generate 100 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(techniques)\n",
    "print(len(topics))\n",
    "# str(list(techniques) )  \n",
    "\", \".join(techniques)  \n",
    "fem_topics = ['menstruation', 'pregnancy', 'childbirth', 'breastfeeding', 'menopause', 'ovulation', 'hormones', 'gynecologist', 'obstetrician', 'midwife', 'doula', 'contraception', 'abortion', 'fertility', 'infertility', 'endometriosis', 'polycystic ovary syndrome', 'uterus', 'ovaries', 'marriage', 'divorce', 'single mother', \n",
    "               'surrogacy','domestic violence', 'sexual assault', 'rape', 'sexual harassment', 'goddess', 'beauty pageant', 'feminism',]\n",
    "count = 0\n",
    "for technique in techniques:\n",
    "    # pick every 10 topics from topics in a loop\n",
    "    for i in range(0, len(topics), 10):\n",
    "        count+=1\n",
    "        # print(topics[i:i+10])\n",
    "        # print(\"Generate 100 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        # print(\"Generate 100 sentences in \"+ lang +\" with action doer is Female. make such sentences such that it can be clearly infered that the action is done by Female using the \"+ technique+\"technique. You can use the topics like \"+ \", \".join(fem_topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        prompts.append(\"Generate 100 sentences in \"+ lang +\" with action doer is Female. make such sentences such that it can be clearly infered that the action is done by Female using the \"+ technique+\"technique. You can use the topics like \"+ \", \".join(fem_topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for technique in techniques:\n",
    "    # pick every 10 topics from topics in a loop\n",
    "    for i in range(0, len(fem_topics), 10):\n",
    "        count+=1\n",
    "        # print(topics[i:i+10])\n",
    "        # print(\"Generate 100 sentences in \"+ lang +\" with action doer as Male or Female. Differentiate between male and female sentences using the \"+ technique +\" technique. You can use the topics like \"+ \", \".join(topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        # print(\"Generate 100 sentences in \"+ lang +\" with action doer is Female. make such sentences such that it can be clearly infered that the action is done by Female using the \"+ technique+\"technique. You can use the topics like \"+ \", \".join(fem_topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "        prompts.append(\"Generate 100 sentences in \"+ lang +\" with action doer is Female. make such sentences such that it can be clearly infered that the action is done by Female using the \"+ technique+\"technique. You can use the topics like \"+ \", \".join(fem_topics[i:i+10]) +\".Return as a python dictionary.\")\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to API:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the openai module is installed\n",
    "# %pip install openai\n",
    "\n",
    "import openai\n",
    "\n",
    "#use the api key\n",
    "openai.api_key = \"sk-2b1fzjzq6q7kz4t7v9z2m6z7kz7z8z7z8\n",
    "# openai.api_key = \n",
    "\n",
    "# call to python prompt\n",
    "resp_dict=[]\n",
    "for p in prompts:\n",
    "    response = openai.ChatCompletion.create(prompt=p, model=\"gpt-3.5-turbo\", max_tokens=1000)\n",
    "    # print(response.choices[0].text)\n",
    "    resp_dict.append(response.choices[0].text)\n",
    "\n",
    "# data_collected=resp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarvam_model = \"sarvam_ai/model_namw\"\n",
    "for p in prompts:\n",
    "    response = sarvam_model.generate_text(p)\n",
    "    resp_dict.append(response)\n",
    "\n",
    "data_collected=resp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the lists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the sentences into 2 lists - female_sents, male_sents\n",
    "# dtatacollected is a list of dictionaries\n",
    "female_sents = []\n",
    "male_sents = []\n",
    "for item in data_collected:\n",
    "    print(item)\n",
    "\n",
    "    # if key 0 is present in item, add the sentences to male_sents\n",
    "    if 0 in item:\n",
    "        female_sents.extend(item[0])\n",
    "\n",
    "    if 1 in item:\n",
    "        male_sents.extend(item[1])\n",
    "    # else:\n",
    "        #nest inside the dict till we get 0 as a key\n",
    "    if 0 not in item and 1 not in item:\n",
    "        for key in item:\n",
    "            print(\"Key:\", key)\n",
    "            if 0 in item[key]:\n",
    "                female_sents.extend(item[key][0])\n",
    "            if 1 in item[key]:\n",
    "                male_sents.extend(item[key][1])\n",
    "\n",
    "print(\"length of female_sents:\", len(female_sents))\n",
    "print(\"length of male_sents:\", len(male_sents))\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in female_data_collected:\n",
    "    for key in item:\n",
    "        female_sents.extend(item[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deduplication of datafram df_data\n",
    "male_sents = list(set(male_sents))\n",
    "female_sents = list(set(female_sents))\n",
    "print(\"length of male_sents after deduplication:\", len(male_sents))\n",
    "print(\"length of female_sents after deduplication:\", len(female_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any overlapping sentences in male_sents and female_sents - print and  remove the common sentences from both lists\n",
    "common_sents = list(set(male_sents).intersection(female_sents))\n",
    "print(\"common sentences:\", \"\\n\".join(common_sents))\n",
    "\n",
    "# remove the common sentences from female_sents\n",
    "female_sents = list(set(female_sents) - set(common_sents))\n",
    "print(\"length of female_sents after removing common sentences: \", len(female_sents))\n",
    "male_sents = list(set(male_sents) - set(common_sents))\n",
    "print(\"length of male_sents after removing common sentences: \", len(male_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime as dt\n",
    "# save the sents and the labels into a csv file with 2 columns - 'hin_text' and 'label' 0-male; 1-female\n",
    "df_data = pd.DataFrame({'hin_text': male_sents + female_sents, 'label': [0]*len(male_sents) + [1]*len(female_sents)})\n",
    "now_str = \"{:%Y_%m_%d_%H_%M_%S}\".format(dt.datetime.now())\n",
    "print(\"now:: \", now_str)\n",
    "df_data.to_csv('../datasets/syn_data_labelled_hin'+now_str+'.csv', index=False,  encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim Further:\n",
    "- Preprocess and collect the data generated by Gemma3\n",
    "- save the data as csv file with 3 columns: hin_text, label, source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import datetime as dt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_str = \"{:%Y_%m_%d_%H_%M_%S}\".format(dt.datetime.now())\n",
    "print(\"now:: \", now_str)\n",
    "\n",
    "# TGT_LANG_CODE = 'hin_Deva'\n",
    "# model_id_tag  = 'gemma_3_12b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  open datasets/outputs_logs copy.txt as read mode txt file to start extracting sentences\n",
    "# mt-gender-steering/datasets/outputs_logs.txt, mt-gender-steering/datasets/hin_outputs_logs.txt\n",
    "# with open('../datasets/outputs_logs.txt', 'r', encoding=\"utf-8\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "# with open('../datasets/'+TGT_LANG_CODE+'_outputs_logs.txt', 'r', encoding=\"utf-8\") as file: \n",
    "with open('hin_Deva_outputs_logs.txt', 'r', encoding=\"utf-8\") as file: \n",
    "\n",
    "    data_temp = file.read()\n",
    "\n",
    "#merge both data and hin_data\n",
    "# data = data + data_temp\n",
    "data = data_temp\n",
    "print(\"shape of data: \", len(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../datasets/hin_outputs_logs2.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "#     data = file.read()\n",
    "\n",
    "code_lists = data.split(\"Output: **<<\")\n",
    "code_lists = code_lists[1:]\n",
    "\n",
    "\n",
    "code_lists[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into list of codes that start with **<<```python and end with >>**\n",
    "code_lists = data.split(\"sentences = \")\n",
    "code_lists = code_lists[1:]\n",
    "\n",
    "print(\"length of code_lists:\", len(code_lists))\n",
    "print(\"code_lists[1]:\", code_lists[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(code_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_list(code_lists):\n",
    "    count=0\n",
    "    data_dict_list = []\n",
    "    for code in code_lists:\n",
    "        # mke a code_temp = code to store the clone of code\n",
    "        code_temp = code\n",
    "        code = code.strip()\n",
    "        code = code.replace(\"...\", \"\")\n",
    "\n",
    "        end_idx = code.find(\"}\")\n",
    "        # print(\"end_idx of }:\", end_idx)\n",
    "        if end_idx == -1:\n",
    "            end_idx = code.find(\">>**\")\n",
    "            code = code[:end_idx]\n",
    "        else: \n",
    "            code = code[:end_idx+1]\n",
    "        code = code.strip()\n",
    "        if not code.endswith('}'):\n",
    "            code = code.strip()\n",
    "            if code.endswith(']'):\n",
    "                code = code + \"}\"\n",
    "            elif code.endswith(\"'],\"):\n",
    "                code = code[:-2] + \"]}\"\n",
    "            elif code.endswith(\"],\"):\n",
    "                code = code[:-2] + \"]}\"\n",
    "            elif code.endswith(','):\n",
    "                code = code[:-1] + \"]}\"\n",
    "            elif code.endswith(\"'\"):\n",
    "                code = code + \"]}\"\n",
    "            elif code.endswith('\"'):\n",
    "                code = code + \"]}\"\n",
    "            elif code.endswith(','):\n",
    "\n",
    "                code = code[:-1] + \"]}\"\n",
    "            \n",
    "            else:\n",
    "                    end_idx = code.rfind('\"')\n",
    "                    # print(\"end_idx of \\\":\", end_idx)\n",
    "                    if end_idx != -1:\n",
    "                        # print(\"Beofre code:\", code)\n",
    "                        # code = code[:end_idx] + '\"]}'\n",
    "                        code = code[:end_idx] + ']}'\n",
    "\n",
    "                        # print(\"After code:\", code)\n",
    "                    else: \n",
    "                        end_idx = code.rfind(\"'\")\n",
    "                        if end_idx != -1:\n",
    "                            code = code[:end_idx] + \"]}\"\n",
    "                        else:\n",
    "                            code = code + \"']}\"\n",
    "                    # print(\"#####code:\", code_temp)\n",
    "                    # print(\"$$$$$code:\", code)\n",
    "\n",
    "        try:\n",
    "            #  dict_code = eval(code\n",
    "            dict_code = ast.literal_eval(code)\n",
    "            data_dict_list.append(dict_code)\n",
    "        except Exception as e:\n",
    "            print(\"\\n\\n! ! ! eval error code:\", code, \"\\n with error : \", e, '\\n code_temp', code_temp)\n",
    "            count+=1\n",
    "            continue\n",
    "             \n",
    "        # break\n",
    "    return data_dict_list, count\n",
    "# data_dict_list, count = get_dict_list(code_lists)\n",
    "\n",
    "# print(\"count of errors:\", count)\n",
    "# print(\"length of data_dict_list:\", len(data_dict_list))\n",
    "# print(\"data_dict_list[0]:\", data_dict_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import possible_indic_relations as poss_indic_rel\n",
    "# import span_encodings as sp_enc\n",
    "# Reload the module to reflect changes\n",
    "importlib.reload(poss_indic_rel)\n",
    "# importlib.reload(sp_enc)\n",
    "import pandas as pd\n",
    "pir= poss_indic_rel.possible_relations\n",
    "pir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indic_sent_dict ={}\n",
    "ambiguous_words = ['grandmother',\n",
    " 'grandfather',\n",
    " 'uncle',\n",
    " 'aunt',\n",
    " 'brother-in-law',\n",
    " 'sister-in-law',\n",
    " 'cousin',\n",
    " 'nephew',\n",
    " 'niece']\n",
    "lang_script_list = [\n",
    "    'hin_Deva', \n",
    "    'guj_Gujr',\n",
    "    'mar_Deva', \n",
    "    'ory_Orya',\n",
    "     'ben_Beng', \n",
    "    'tam_Taml', \n",
    "    'pan_Guru',\n",
    "     'tel_Telu',\n",
    "      'mal_Mlym', 'kan_Knda',]\n",
    "\n",
    "lang_code_map = {\n",
    "    'eng_Latn': 'Eng',\n",
    "    'hin_Deva': 'Hin',\n",
    "    'guj_Gujr': 'Guj',\n",
    "    'kan_Knda': 'Kan',\n",
    "    'mal_Mlym': 'Mal',\n",
    "    'mar_Deva': 'Mar',\n",
    "    'tam_Taml': 'Tam',\n",
    "    'tel_Telu': 'Tel',\n",
    "    'pan_Guru': 'Pun',\n",
    "    'ben_Beng': 'Ben',\n",
    "    'ory_Orya': 'Odi'\n",
    "}\n",
    "\n",
    "for word in ambiguous_words:\n",
    "    indic_sent_dict[word] = {}\n",
    "    for lang_code in lang_script_list:\n",
    "        indic_sent_dict[word][lang_code] = {}\n",
    "        for key in pir[word][lang_code]:\n",
    "            indic_sent_dict[word][lang_code][key] = []\n",
    "            \n",
    "\n",
    "indic_sent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "\n",
    "sentences = [\"ମୋର ବଡ଼ ମାଆ ଘରେ ଅଛନ୍ତି\", \"ସେ ବଡ ମା ସହିତ ଆସିଲେ\", \"ମୁଁ ମାଉସୀଁ ସହ ଗଲି\"]\n",
    "key = \"ବଡ ମା\"\n",
    "\n",
    "threshold = 80  # Adjust based on strictness\n",
    "count = sum(fuzz.partial_ratio(key, sent) >= threshold for sent in sentences)\n",
    "print(\"Count:\", count)\n",
    "\n",
    "\n",
    "def fuzzy_match(key, sentence, threshold=65):\n",
    "    \"\"\"Check if the key is present in the sentence with a given threshold.\"\"\"\n",
    "    score = fuzz.partial_ratio(key, sentence)\n",
    "    # print(\"Score:\", score)\n",
    "    return score, score >= threshold\n",
    "\n",
    "print(fuzzy_match(\"ବଡ ମା\", \"ମୋର ବଡ଼ ମା ଘରେ ଅଛନ୍ତି\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dict_list), data_dict_list[980:982]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search if दादा is present in data_dict_list, count instances, get 1st index and print\n",
    "for d in data_dict_list:\n",
    "    for key in d:\n",
    "        if key == \"दादा\":\n",
    "            print(\"key:\", key)\n",
    "            print(\"value:\", d[key])\n",
    "            print(\"count:\", len(d[key]))\n",
    "            print(\"first index:\", d[key][0])\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_key_NF =0\n",
    "key_nf_set = set()\n",
    "for lang in lang_script_list:\n",
    "#   if lang == 'kan_Knda':\n",
    "    print(\"lang:\", lang)\n",
    "    data = None\n",
    "    with open(lang+'_outputs_logs.txt', 'r', encoding=\"utf-8\") as file: \n",
    "        data = file.read()\n",
    "    # print(\"shape of data: \", len(data), \"for lang:\", lang)\n",
    "\n",
    "    # data = data[:5000]\n",
    "    \n",
    "    code_lists = data.split(\"Output: **<<\")\n",
    "    code_lists = code_lists[1:]\n",
    "    code_lists = data.split(\"sentences = \")\n",
    "    code_lists = code_lists[1:]\n",
    "\n",
    "    # print(\"length of code_lists:\", len(code_lists))\n",
    "    # print(\"code_lists[1]:\", code_lists[1])\n",
    "\n",
    "    data_dict_list, count = get_dict_list(code_lists)\n",
    "    print(\"count of errors:\", count, type(data_dict_list), type(data_dict_list[0]), lang)\n",
    "    # print(\"length of data_dict_list:\", len(data_dict_list))\n",
    "    # print(\"data_dict_list[0]:\", data_dict_list[0])\n",
    "\n",
    "    for data_dict in data_dict_list:\n",
    "        for key in data_dict:\n",
    "            # data_dict[key] = ast.literal_eval(data_dict[key])\n",
    "            # print(type(data_dict[key]))\n",
    "            # if len(data_dict[key]) >10:\n",
    "            #     print(\"key:\", key)\n",
    "            #     print(\"@@@@@@@data_dict[key]:\", data_dict[key])\n",
    "            flag = False\n",
    "            for word in ambiguous_words:\n",
    "                if key in indic_sent_dict[word][lang]:\n",
    "                    indic_sent_dict[word][lang][key].extend(data_dict[key])\n",
    "                    flag = True\n",
    "                    break\n",
    "                else:\n",
    "                    # check for which indic_sent_dict[word][lang].keys() the key has max score\n",
    "                    # if the key is not present in indic_sent_dict[word][lang], check for fuzzy match\n",
    "                    max_score = -1\n",
    "                    max_key = None\n",
    "                    for k in indic_sent_dict[word][lang].keys():\n",
    "                        score, match = fuzzy_match(k, key, threshold=65)\n",
    "                        if match and score > max_score:\n",
    "                            max_score = score\n",
    "                            max_key = k\n",
    "                    if max_key is not None:\n",
    "                        indic_sent_dict[word][lang][max_key].extend(data_dict[key])\n",
    "                        flag = True\n",
    "                        break \n",
    "            if not flag:\n",
    "                # print(\"-----------key not found in indic_sent_dict:\", key, lang)\n",
    "                count_key_NF+=1\n",
    "                key_nf_set.add(key)\n",
    "    # save the indic_sent_dict to a txt file in append mode\n",
    "    with open('indic_sent_dict.txt', 'a', encoding=\"utf-8\") as file:\n",
    "        file.write(str(indic_sent_dict))\n",
    "        file.write(\"\\n\\n\\n\")\n",
    "\n",
    "# print the indic_sent_dict\n",
    "# print(\"indic_sent_dict:\", indic_sent_dict)\n",
    "# \n",
    "len(key_nf_set), key_nf_set\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in indic_sent_dict.keys():\n",
    "    for tgt_lang in lang_script_list:\n",
    "        print(\"Word: \", word, \"TGT_LANG: \", tgt_lang)\n",
    "        sum =0\n",
    "        for k in indic_sent_dict[word][tgt_lang].keys():\n",
    "            sum += len(indic_sent_dict[word][tgt_lang][k])\n",
    "            print(\"    key: \", k, \" len: \", len(indic_sent_dict[word][tgt_lang][k]))\n",
    "        print(\"==Total: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "indic_sent_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save indic_sent_dict to a txt file\n",
    "with open('indic_sent_dict__.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(str(indic_sent_dict))\n",
    "    file.write(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "for word in indic_sent_dict:\n",
    "    for lang_code in indic_sent_dict[word]:\n",
    "        for key in indic_sent_dict[word][lang_code]:\n",
    "            sents = indic_sent_dict[word][lang_code][key]\n",
    "            for i in tqdm.tqdm(range(0, len(sents), batch_size)):\n",
    "                batch = sents[i:i+batch_size]\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### labeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df_data from data_dict_list with columns - 'hin_text' and 'label' and source. text is the list string, keys are label and source is 'gemma3-12b'\n",
    "df_data = pd.DataFrame()\n",
    "tgt_lang_txt_list =[]\n",
    "labels=[]\n",
    "for data_dict in data_dict_list:\n",
    "    for key in data_dict:\n",
    "        tgt_lang_txt_list.extend(data_dict[key])\n",
    "        labels.extend([key]*len(data_dict[key]))\n",
    "\n",
    "source_list=[model_id_tag]*len(tgt_lang_txt_list)\n",
    "df_data[TGT_LANG_CODE+'_text'] = tgt_lang_txt_list\n",
    "df_data['label'] = labels\n",
    "df_data['source'] = source_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data.head(20))\n",
    "print(df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(now_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates from df_data subsets TGT_LANG_CODE+'_text', 'label', 'source'\n",
    "df_data = df_data.drop_duplicates(subset=[TGT_LANG_CODE+'_text', 'label', 'source'], keep='first')\n",
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['label'] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of duplicates in column df_data[TGT_LANG_CODE+'_text']\n",
    "df_data[TGT_LANG_CODE+'_text'].duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what are duplicates in df_data[TGT_LANG_CODE+'_text']\n",
    "df_data[TGT_LANG_CODE+'_text'][df_data[TGT_LANG_CODE+'_text'].duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates from df_data[TGT_LANG_CODE+'_text']\n",
    "df_data = df_data.drop_duplicates(subset=[TGT_LANG_CODE+'_text'], keep='first')\n",
    "print(\"length of df_data after removing duplicates:\", df_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the duplicated df_data[TGT_LANG_CODE+'_text'] to a csv file with all columns\n",
    "t = df_data[df_data[TGT_LANG_CODE+'_text'].duplicated()]\n",
    "# sort t as per the column TGT_LANG_CODE+'_text'\n",
    "t = t.sort_values(by=[TGT_LANG_CODE+'_text'])\n",
    "print(t.shape, now_str)\n",
    "t.to_csv('../datasets/duplicates_hin'+now_str+'.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are no duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_data.to_csv('../datasets/syn_verb_data_3labelled_'+TGT_LANG_CODE+'_'+ now_str+'.csv', index=False,  encoding=\"utf-8\")\n",
    "df_data.to_csv('../datasets/hin_verbs_syn_data_3labelled.csv', index=False,  encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"../datasets/syn_data_3labelled_hin_2025_03_28_18_41_18.csv\", encoding=\"utf-8\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data.duplicated()\n",
    "# show what are the duplicate rows\n",
    "duplicates = df_data[df_data.duplicated(subset=[TGT_LANG_CODE+'_text'], keep=False)]\n",
    "print(\"duplicates:\", duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates_df from df_temp\n",
    "\n",
    "df_cleaned = df_temp.loc[~df_temp.index.isin(duplicates.index)].reset_index(drop=True)\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show duplicate rows side by side\n",
    "duplicates = duplicates.sort_values(by=[TGT_LANG_CODE+'_text'])\n",
    "duplicates = duplicates.reset_index(drop=True)\n",
    "print(\"duplicates:\", duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if every row is repeated 3 times\n",
    "# duplicates['hin_text'].value_counts()[duplicates['hin_text'].value_counts() == 3]\n",
    "# Step 1: Get texts that appear exactly 3 times\n",
    "valid_texts = duplicates['hin_text'].value_counts()[duplicates['hin_text'].value_counts() == 3].index\n",
    "\n",
    "# Step 2: Filter rows with those texts\n",
    "resolved_dup_df = duplicates[duplicates['hin_text'].isin(valid_texts)].copy()\n",
    "resolved_dup_df = resolved_dup_df[resolved_dup_df['label'] == 2]\n",
    "resolved_dup_df = resolved_dup_df.reset_index(drop=True)\n",
    "resolved_dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove resolved_dup_df from duplicates if hin_text is same\n",
    "duplicates_remaining = duplicates[~duplicates['hin_text'].isin(resolved_dup_df['hin_text'])].copy()\n",
    "duplicates_remaining = duplicates_remaining.reset_index(drop=True)\n",
    "duplicates_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many such texts even exist with label 2 in full DataFrame\n",
    "# print(duplicates_remaining[duplicates_remaining['label'] == 2]['hin_text'].isin(target_texts).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'hin_text' and collect unique labels\n",
    "label_sets = duplicates_remaining.groupby('hin_text')['label'].apply(set)\n",
    "print(label_sets)\n",
    "\n",
    "# Step 2: Filter 'hin_text's with labels {0, 1} only\n",
    "target_texts = label_sets[(label_sets.apply(lambda x: 0 in x and 1 in x and 2 not in x))].index\n",
    "print(\"target_texts:\", target_texts)    \n",
    "\n",
    "# Step 3: From duplicates, get rows with those texts and label 2\n",
    "label_2_rows = duplicates_remaining[(duplicates_remaining['hin_text'].isin(target_texts)) ].copy()\n",
    "\n",
    "print(\"label_2_rows:\", label_2_rows)\n",
    "\n",
    "# Step 4: Remove those rows from original duplicates\n",
    "duplicates_remaining = duplicates_remaining.drop(label_2_rows.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns in label_2_rows:\", label_2_rows.columns, \"label_2_rows:\", label_2_rows.shape)\n",
    "print(\"Columns in resolved_dup_df:\", resolved_dup_df.columns, \"resolved_dup_df:\", resolved_dup_df.shape)\n",
    "\n",
    "# drop row with duplicate values in 'hin_text' from label_2_rows\n",
    "label_2_rows = label_2_rows.drop_duplicates(subset=[TGT_LANG_CODE+'_text'], keep='first')\n",
    "label_2_rows['label'] = 2\n",
    "\n",
    "resolved_dup_df = pd.concat([resolved_dup_df, label_2_rows], ignore_index=True, axis=0)\n",
    "resolved_dup_df = resolved_dup_df.reset_index(drop=True)\n",
    "\n",
    "print(\"resolved_dup_df:\", resolved_dup_df.shape)\n",
    "print(\"duplicates:\", duplicates.shape)\n",
    "print(\"duplicates_remaining:\", duplicates_remaining.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove {0,2}\n",
    "\n",
    "# Step 2: Filter 'hin_text's with labels {0, 1} only\n",
    "target_texts = label_sets[(label_sets.apply(lambda x: 0 in x and 1 not in x and 2  in x))].index\n",
    "print(\"target_texts:\", target_texts)    \n",
    "\n",
    "# Step 3: From duplicates, get rows with those texts and label 2\n",
    "label_1_rows = duplicates_remaining[(duplicates_remaining['hin_text'].isin(target_texts)) ].copy()\n",
    "\n",
    "print(\"label_1_rows:\", label_1_rows)\n",
    "\n",
    "# Step 4: Remove those rows from original duplicates\n",
    "duplicates_remaining = duplicates_remaining.drop(label_1_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Columns in label_1_rows:\", label_1_rows.columns, \"label_1_rows:\", label_1_rows.shape)\n",
    "print(\"Columns in resolved_dup_df:\", resolved_dup_df.columns, \"resolved_dup_df:\", resolved_dup_df.shape)\n",
    "\n",
    "# drop row with duplicate values in 'hin_text' from label_1_rows\n",
    "label_1_rows = label_1_rows.drop_duplicates(subset=[TGT_LANG_CODE+'_text'], keep='first')\n",
    "label_1_rows['label'] = 2\n",
    "\n",
    "resolved_dup_df = pd.concat([resolved_dup_df, label_1_rows], ignore_index=True, axis=0)\n",
    "resolved_dup_df = resolved_dup_df.reset_index(drop=True)\n",
    "\n",
    "print(\"resolved_dup_df:\", resolved_dup_df.shape)\n",
    "print(\"duplicates:\", duplicates.shape)\n",
    "print(\"duplicates_remaining:\", duplicates_remaining.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining = duplicates_remaining.drop_duplicates(subset=[TGT_LANG_CODE+'_text'], keep='first')\n",
    "duplicates_remaining.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicates_remaining sort by hin_text\n",
    "duplicates_remaining = duplicates_remaining.sort_values(by=[TGT_LANG_CODE+'_text'])\n",
    "duplicates_remaining = duplicates_remaining.reset_index(drop=True)\n",
    "duplicates_remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining.to_csv('../datasets/duplicates_remaining_'+TGT_LANG_CODE+'_'+ now_str+'.csv', index=False,  encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_remaining = duplicates_remaining.sort_values(by=['hin_text']).reset_index(drop=True)\n",
    "duplicates_remaining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_dup_df = resolved_dup_df.sort_values(by=['hin_text']).reset_index(drop=True)\n",
    "resolved_dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_dup_df = pd.concat([resolved_dup_df, duplicates_remaining], ignore_index=True, axis=0)\n",
    "resolved_dup_df = resolved_dup_df.reset_index(drop=True)\n",
    "resolved_dup_df = resolved_dup_df.sort_values(by=['hin_text']).reset_index(drop=True)\n",
    "resolved_dup_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.concat([df_cleaned, resolved_dup_df], ignore_index=True, axis=0)\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned = df_cleaned.sort_values(by=[TGT_LANG_CODE+'_text']).reset_index(drop=True)\n",
    "df_cleaned = df_cleaned.drop_duplicates(subset=[TGT_LANG_CODE+'_text'], keep='first')\n",
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv('../datasets/syn_data_3labelled_cleaned_'+TGT_LANG_CODE+'_'+ now_str+'.csv', index=False,  encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim :: Same label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read \n",
    "df1 = pd.read_csv('../datasets/syn_data_3labelled_cleaned_hin_2025_04_11_18_28_56.csv', header=0)\n",
    "\n",
    "df2 = pd.read_csv('../datasets/syn_data_3labelled_hin_2025-03-30_10-53-38.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new df df3 that has same label and gemma_3_12b_label from df2\n",
    "df_same_labels = df2.copy()\n",
    "df_same_labels = df_same_labels[df_same_labels['label'] == df_same_labels['gemma3_12b_label']]\n",
    "df_same_labels = df_same_labels.reset_index(drop=True)\n",
    "df_same_labels.head()\n",
    "df_same_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same_labels.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_same_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentag eof df2 is df3 by rows\n",
    "percentage = (df_same_labels.shape[0] / df2.shape[0]) * 100\n",
    "print(\"percentage of df2 is df3 by rows:\", percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff_labels = df2.copy()\n",
    "df_diff_labels = df_diff_labels[df_diff_labels['label'] != df_diff_labels['gemma3_12b_label']]\n",
    "df_diff_labels = df_diff_labels.reset_index(drop=True)\n",
    "# df_diff_labels.head()\n",
    "df_diff_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"percentage of df2 is df_diff_labels by rows:\", (df_diff_labels.shape[0] / df2.shape[0]) * 100)\n",
    "df_diff_labels['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by label and gemma_3_12b_label and count the number of rows\n",
    "df_diff_labels.groupby(['label', 'gemma3_12b_label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is df1 a subset of df2 only by comparing the strip of column 'hin_text' \n",
    "df1['hin_text'] = df1['hin_text'].str.strip()\n",
    "df2['hin_text'] = df2['hin_text'].str.strip()\n",
    "# now compare the two dataframes find the overlapping rows of hin_text\n",
    "df1_hin_text = df1['hin_text'].tolist()\n",
    "df2_hin_text = df2['hin_text'].tolist()\n",
    "df1_hin_text_set = set(df1_hin_text)\n",
    "df2_hin_text_set = set(df2_hin_text)\n",
    "overlapping_hin_text = df1_hin_text_set.intersection(df2_hin_text_set)\n",
    "print(\"overlapping_hin_text:\", overlapping_hin_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract train.src_lang+train.tgt_lang for FT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in indic_sent_dict.keys():\n",
    "    for tgt_lang in lang_script_list[:2]:\n",
    "        print(\"Word: \", word, \"TGT_LANG: \", tgt_lang)\n",
    "        # sum =0\n",
    "        for k in indic_sent_dict[word][tgt_lang].keys():\n",
    "            # sum += len(indic_sent_dict[word][tgt_lang][k])\n",
    "            print(\"    key: \", k, \" len: \", len(indic_sent_dict[word][tgt_lang][k]) , \"\\n\\n\")\n",
    "            for sent in indic_sent_dict[word][tgt_lang][k][:50]:\n",
    "                print(word+','+tgt_lang+','+k+','+sent)\n",
    "        # print(\"==Total: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_ind_sents = pd.read_csv(\"/home/sofia/git_repos/Multilingual_project/notebooks/eng_indic_texts.csv\", header=0, encoding=\"utf-8\")\n",
    "eng_ind_sents = pd.read_csv(\"/data/users/sofia/eng_indic_texts.csv\", header=0, encoding=\"utf-8\")\n",
    "eng_ind_sents = eng_ind_sents.dropna(subset=['eng_text', 'tgt_lang_text'])\n",
    "eng_ind_sents = eng_ind_sents.drop_duplicates( keep='first')\n",
    "eng_ind_sents.head()\n",
    "print(\"shape of eng_ind_sents:\", eng_ind_sents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_ind_sents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for word in ambiguous_words:\n",
    "    for tgt_lang in lang_script_list:\n",
    "      # if tgt_lang in ['hin_Deva', 'guj_Gujr', ]:\n",
    "        # print(\"Word: \", word, \"TGT_LANG: \", tgt_lang)\n",
    "        count_list =[]\n",
    "        for k in pir[word][tgt_lang].keys():\n",
    "            # check if same word, lang, k for eng_indic_texts has value >=50\n",
    "            t = eng_ind_sents[(eng_ind_sents['word'] == word) & (eng_ind_sents['tgt_lang'] == tgt_lang) & (eng_ind_sents['indic_term'] == k)]\n",
    "            count_list.append(t.shape[0])\n",
    "            if t.shape[0] ==0:\n",
    "                count+=1\n",
    "                print(f\"Word: {word}, TGT_LANG: {tgt_lang}, Key: {k}\")# does not have sufficient data.\", t.shape[0])\n",
    "            \n",
    "\n",
    "        # print(\"Word: \", word, \"TGT_LANG: \", tgt_lang, \"Count List: \", count_list, \"Total Count:\", sum(count_list))\n",
    "        # if np.std(count_list, ddof=1) >5:\n",
    "        #     print(f\"Word: {word}, TGT_LANG: {tgt_lang} has high variance in counts across keys. Standard Deviation: {np.std(count_list, ddof=1)}\") \n",
    "        #     print(\"keys: \"  , pir[word][tgt_lang].keys(), count_list)\n",
    "        \n",
    "\n",
    "        # print no.of lines to  delete  the extra lines apart from min (count_list) from each key also print respective key\n",
    "        # min_count = min(count_list)\n",
    "        # for k, c in zip(pir[word][tgt_lang].keys(), count_list):\n",
    "        #     if c > min_count:\n",
    "        #         print(f\"Word: {word}, TGT_LANG: {tgt_lang}, Key: {k} has {c} lines. Need to delete {c - min_count} lines.\")\n",
    "            # else:\n",
    "            #     print(f\"Word: {word}, TGT_LANG: {tgt_lang}, Key: {k} has sufficient data with {c} lines.\")\n",
    "\n",
    "# print(\"Total count of insufficient data for any word, tgt_lang, key:\", count)\n",
    "count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for all rows in eng_indic_texts if word is present in eng_indic_texts['eng_text'] and\n",
    "# indic_term is present in eng_indic_texts['tgt_lang_text']\n",
    "# if not then print the row and delete the row\n",
    "print(\"shape of eng_indic_texts before filtering:\", eng_ind_sents.shape)\n",
    "for index, row in eng_ind_sents.iterrows():\n",
    "    # Check for NaN before using 'in'\n",
    "    tgt_lang_text = row['tgt_lang_text']\n",
    "    eng_text = row['eng_text']\n",
    "    indic_term = row['indic_term']\n",
    "    tgt_lang = row['tgt_lang']\n",
    "    word = row['word']\n",
    "    if pd.isnull(tgt_lang_text) or pd.isnull(indic_term) or fuzz.partial_ratio(indic_term, str(tgt_lang_text).lower()) < 80:\n",
    "        print(\"indic term not found in tgt_lang_text:\", indic_term, \"in row:\", tgt_lang_text)\n",
    "        eng_ind_sents.drop(index, inplace=True)\n",
    "        continue  # skip further checks if already dropped\n",
    "    if pd.isnull(eng_text) or pd.isnull(word) or word not in str(eng_text).lower():\n",
    "        print(\"--\", f\"{word},{tgt_lang},{indic_term},{tgt_lang_text},{eng_text}\" )\n",
    "        eng_ind_sents.drop(index, inplace=True)\n",
    "\n",
    "    # if indic_term not in pir[word][tgt_lang].keys():\n",
    "    if not process.extractOne(indic_term, pir[word][tgt_lang].keys(), score_cutoff=80):\n",
    "\n",
    "        print(\"indic_term not found in pir[word][\",tgt_lang,\"].keys():\", indic_term, \"in row:\", tgt_lang_text, \"keys:\", pir[word][tgt_lang].keys(), indic_term)\n",
    "        eng_ind_sents.drop(index, inplace=True)\n",
    "\n",
    "print(\"shape of eng_indic_texts after filtering:\", eng_ind_sents.shape)\n",
    "eng_ind_sents.to_csv('/data/users/sofia/eng_indic_texts.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blaancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# df_agg_resampled = pd.DataFrame()\n",
    "# for lang in lang_script_list:\n",
    "\n",
    "#     for word in eng_ind_sents[eng_ind_sents['tgt_lang']== lang]['indic_term'].unique():\n",
    "#         print(\"Word: \", word)\n",
    "#         # get the rows with word\n",
    "#         lang_df = eng_ind_sents[eng_ind_sents['tgt_lang']== lang]\n",
    "#         word_rows = lang_df[lang_df['indic_term'] == word]\n",
    "#         print(\"shape of word_rows:\", word_rows.shape)\n",
    "#         word_rows = resample(word_rows, replace=True, n_samples=50, random_state=42)\n",
    "#         print(\"shape of word_rows after resampling:\", word_rows.shape)\n",
    "#         # save the rows to a csv file\n",
    "#         # word_rows.to_csv(f\"eng_indic_{word}.csv\", index=False, encoding=\"utf-8\")\n",
    "#         df_agg_resampled = pd.concat([df_agg_resampled, word_rows], ignore_index=True, axis=0)\n",
    "# print(\"shape of df_agg_resampled:\", df_agg_resampled.shape, df_agg_resampled.groupby(['indic_term', ]).size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(df, col, num):\n",
    "    \"\"\"\n",
    "    Balance the dataframe by resampling the specified column to have a fixed number of samples.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input dataframe.\n",
    "    col (str): The column to balance.\n",
    "    num (int): The number of samples to balance to.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The balanced dataframe.\n",
    "    \"\"\"\n",
    "    balanced_df = pd.DataFrame()\n",
    "    # print(\"Before the balancing, shape of df:\", df.shape, df.groupby([col]).size())\n",
    "    for value in df[col].unique():\n",
    "        subset = df[df[col] == value]\n",
    "        if len(subset) < num:\n",
    "            subset = resample(subset, replace=True, n_samples=num, random_state=42)\n",
    "        else:\n",
    "            subset = subset.sample(n=num, random_state=42)\n",
    "        balanced_df = pd.concat([balanced_df, subset], ignore_index=True, axis=0)\n",
    "        # print(f\"After balancing for {value}, shape of balanced_df: {balanced_df.shape}, count: {balanced_df[col].value_counts()[value]}\")\n",
    "    # print(\"Final shape of balanced_df:\", balanced_df.shape, balanced_df.groupby([col]).size())\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segragate the sentences into groups of tgt_lang and make train.{tgt_lang} text file of tgt_lang_text and\n",
    "# train.eng_Latn for eng_text\n",
    "tgt_langs = ['hin_Deva', 'guj_Gujr', 'mar_Deva', 'ory_Orya', 'ben_Beng', 'tam_Taml', 'pan_Guru', 'tel_Telu', 'mal_Mlym', 'kan_Knda']\n",
    "for tgt_lang in tgt_langs:\n",
    "    # make a new df with only the tgt_lang and eng_Latn columns\n",
    "    df_tgt_lang = eng_ind_sents[ eng_ind_sents['tgt_lang'] == tgt_lang]#[['tgt_lang_text', 'eng_text']]\n",
    "    # df_tgt_lang = df_agg_resampled[ df_agg_resampled['tgt_lang'] == tgt_lang]#[['tgt_lang_text', 'eng_text']]\n",
    "\n",
    "    df_tgt_lang = df_tgt_lang.dropna()\n",
    "    df_tgt_lang = df_tgt_lang.reset_index(drop=True)\n",
    "    # split the df_tgt_lang into train and test sets using train_test_split\n",
    "    if df_tgt_lang.empty:\n",
    "        print(\"DataFrame is empty. Skipping train-test split.\", tgt_lang    )\n",
    "    else:\n",
    "        # print(\"shape of df_tgt_lang:\", df_tgt_lang.shape, \"for tgt_lang:\", tgt_lang)\n",
    "        # print(\"Shape of word wise df_tgt_lang:\", df_tgt_lang.groupby('indic_term').size(), df_tgt_lang.shape, \"for tgt_lang:\", tgt_lang)\n",
    "        df_train, df_test = train_test_split(df_tgt_lang, test_size=0.2, random_state=42, shuffle=True, stratify=df_tgt_lang['indic_term'])\n",
    "        df_train = balance_df(df_train, 'indic_term', 40)\n",
    "        df_test = balance_df(df_test, 'indic_term', 10)\n",
    "\n",
    "# assert that df_train when grouped by indic term, all groups have same no.of rows = 40\n",
    "        # print(\"shape of df_train:\", df_train.shape, \"for tgt_lang:\", tgt_lang)\n",
    "        print(\"Shape of word wise df_train:\", df_train.groupby('indic_term').size(), df_train.shape, \"for tgt_lang:\", tgt_lang)\n",
    "        # print(\"shape of df_test:\", df_test.shape, \"for tgt_lang:\", tgt_lang)    \n",
    "        # assert df_train.groupby('indic_term').size().min() == 40, f\"Not all groups in df_train have 40 rows for {tgt_lang}\"\n",
    "        # assert df_test.groupby('indic_term').size().min() == 10, f\"Not all groups in df_test have 10 rows for {tgt_lang}\"\n",
    "        \n",
    "        \n",
    "        # # save the df_tgt_lang to a text file - /data/users/sofia/indictrans2_FT/en_ind_FT\n",
    "        with open('/data/users/sofia/indictrans2_FT/en_ind_FT/train/eng_Latn-' + tgt_lang+ '/train.' + tgt_lang, 'w', encoding='utf-8') as f:\n",
    "            for index, row in df_train.iterrows():\n",
    "                f.write(row['tgt_lang_text'] + '\\n')\n",
    "        with open('/data/users/sofia/indictrans2_FT/en_ind_FT/train/eng_Latn-'+tgt_lang+'/train.eng_Latn', 'w', encoding='utf-8') as f:\n",
    "            for index, row in df_train.iterrows():\n",
    "                f.write(row['eng_text'] + '\\n')\n",
    "\n",
    "        with open('/data/users/sofia/indictrans2_FT/en_ind_FT/devtest/all/eng_Latn-' + tgt_lang+ '/dev.' + tgt_lang, 'w', encoding='utf-8') as f:\n",
    "            for index, row in df_test.iterrows():\n",
    "                f.write(row['tgt_lang_text'] + '\\n')\n",
    "        with open('/data/users/sofia/indictrans2_FT/en_ind_FT/devtest/all/eng_Latn-'+tgt_lang+'/dev.eng_Latn', 'w', encoding='utf-8') as f:\n",
    "            for index, row in df_test.iterrows():\n",
    "                f.write(row['eng_text'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking for more prompted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_ind_sents = pd.read_csv(\"/home/sofia/git_repos/Multilingual_project/notebooks/eng_indic_texts.csv\", header=0, encoding=\"utf-8\")\n",
    "# eng_ind_sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = eng_indic_texts.groupby(['word', 'tgt_lang', 'indic_term']).size()\n",
    "# categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
