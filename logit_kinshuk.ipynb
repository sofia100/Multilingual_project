{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAM_SIZE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resolve_logits_for_best_beam(outputs, num_beams):\n",
    "    \"\"\" Resolve the logits from the best beam, using model output from a generate call.\n",
    "        For a shape [tokens?, batch_size*num_beams, vocab], returns [tokens?, batch_size, vocab]\n",
    "\n",
    "        Assumes num_return_sequences=1.\"\"\"\n",
    "\n",
    "    best_logits  = []\n",
    "    beam_indices = [ outputs.beam_indices[:,i].tolist() for i in range(len(outputs.logits)) ]\n",
    "\n",
    "    for beam_index, logits in zip(beam_indices, outputs.logits):\n",
    "        beam_index = [ idx if idx != -1 else ((num_beams*(i+1))-1) for i, idx in enumerate(beam_index) ]\n",
    "        best_logits.append(logits[beam_index,:])\n",
    "\n",
    "    return best_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofia/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-small\", torch_dtype='bfloat16', device_map='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inputs = [\n",
    "    \"Translate English to French: The capital of India is New Delhi.\",\n",
    "    \"Translate English to German: The city of Delhi is very polluted these days.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"La capital de l'Inde est New Delhi.\".index(\"Delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_logits_for_span(logits, sequences, tokenizer, search_spans):\n",
    "    \"\"\" Given search spans, returns the logits before the span was generated.\n",
    "\n",
    "    Args:\n",
    "        logits (tuple[Tensor]): Tuple of tensors, of shape [tokens?, batch_size, vocab]\n",
    "        sequences (tuple[list[int]]): Tokenized output sequences.\n",
    "        tokenizer (PreTrainedTokenizerBase): Tokenizer for the model.\n",
    "        search_spans (list[str]): batch_size spans to search for. Must be present in the generated sequences.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of shape [batch_size, vocab] indicating the logits before the span for each batch element.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"seq  \", sequences)\n",
    "\n",
    "    if isinstance(search_spans, str):\n",
    "        search_spans = [ search_spans ] * len(sequences)\n",
    "\n",
    "    # print(\"spans\", search_spans)\n",
    "\n",
    "    detok_outputs = tokenizer.batch_decode(sequences, skip_special_tokens=True)\n",
    "    print(\"detok\", detok_outputs)\n",
    "    \n",
    "\n",
    "    positions = [ output.index(span) for output, span in zip(detok_outputs, search_spans) ]\n",
    "    print(\"pos  \", positions)\n",
    "    logit_pos = [  ]\n",
    "\n",
    "    for seq, detok_seq, span, pos in zip(sequences, detok_outputs, search_spans, positions):\n",
    "        if pos == 0:\n",
    "            subtokens = tokenizer(span, add_special_tokens=False).input_ids\n",
    "            print(\"subtok if 1\", subtokens)\n",
    "        else:\n",
    "            subtokens = tokenizer(span, add_special_tokens=False).input_ids\n",
    "            print(\"subtok else 1\", subtokens)\n",
    "            subtokens_2 = tokenizer(detok_seq[pos-1] + span, add_special_tokens=False).input_ids\n",
    "            print(\"subtok else 2\", subtokens_2)\n",
    "            if subtokens[0] not in subtokens_2: subtokens = subtokens_2\n",
    "\n",
    "        print(\"subtok effective\", subtokens)\n",
    "        print(\"seq::\", seq)\n",
    "        \n",
    "\n",
    "        idx = 0\n",
    "        while idx < len(seq):\n",
    "            if all(seq[idx+i] == tok for i, tok in enumerate(subtokens)): \n",
    "                print(\"found\", idx)\n",
    "                break\n",
    "            idx += 1\n",
    "        logit_pos.append(idx)\n",
    "        print(\"subtok\", subtokens, \"idx\", idx, \"logit_pos\", logit_pos)\n",
    "\n",
    "    return torch.stack([ logits[token][batch,:] for batch, token in enumerate(logit_pos) ])\n",
    "    # return torch.gather(output.scores, index=, dim=-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofia/anaconda3/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output y_hat: [\"La capital de l'Inde est New Delhi.\", 'Das Stadt von Delhi ist sehr polluted diesen Tagen.']\n",
      "detok [\"La capital de l'Inde est New Delhi.\", 'Das Stadt von Delhi ist sehr polluted diesen Tagen.']\n",
      "pos   [29, 14]\n",
      "subtok else 1 [10619]\n",
      "subtok else 2 [10619]\n",
      "subtok effective [10619]\n",
      "seq:: tensor([    0,   325,  1784,    20,     3,    40,    31, 26267,   259,   368,\n",
      "        10619,     5,     1,     0,     0], device='cuda:7')\n",
      "found 10\n",
      "subtok [10619] idx 10 logit_pos [10]\n",
      "subtok else 1 [10619]\n",
      "subtok else 2 [10619]\n",
      "subtok effective [10619]\n",
      "seq:: tensor([    0,   644,  3287,   193, 10619,   229,  1319,  5492,  2810,    26,\n",
      "            3,  5162, 13657,     5,     1], device='cuda:7')\n",
      "found 4\n",
      "subtok [10619] idx 4 logit_pos [10, 4]\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode(): #INFERENCE MODE TODO??\n",
    "    inputs_t = tokenizer(inputs, padding='longest', return_tensors='pt')\n",
    "    outputs = model.generate(\n",
    "        **inputs_t.to(model.device),\n",
    "        max_new_tokens=20,\n",
    "        do_sample=False,\n",
    "        temperature=0,\n",
    "        output_scores=True, # must be true since beam indices are needed\n",
    "        output_logits=True,\n",
    "        return_dict_in_generate=True,\n",
    "        num_beams=BEAM_SIZE,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    del outputs.scores\n",
    "    outputs.beam_indices = outputs.beam_indices.cpu()\n",
    "    # print(\"Beam Indices:\",len(outputs.beam_indices[0]), outputs.beam_indices)    \n",
    "    outputs.logits = tuple(logits.cpu() for logits in outputs.logits)\n",
    "    # print(\"Length:\", len(outputs.logits))\n",
    "    # print(\"Output Logits:\",  outputs.logits)\n",
    "\n",
    "\n",
    "print(\"model output y_hat:\", tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "\n",
    "# To get logits for the generation before Delhi:\n",
    "\n",
    "# 1. Resolve logits for the best beam from these inputs\n",
    "#    (would be a tuple of tensoors of shape of [tokens?, batch_size, vocab])\n",
    "best_logits = resolve_logits_for_best_beam(outputs, num_beams=BEAM_SIZE)\n",
    "\n",
    "# 2. Get the logits before New Delhi\n",
    "#    (would be a tensor of shape of [batch_size, vocab])\n",
    "start_logits = get_logits_for_span(best_logits, outputs.sequences, tokenizer, [ \"Delhi\", \"Delhi\" ])\n",
    "\n",
    "# 3. Use these for experiments ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logits: [tensor([[-43.5000,   0.1719,  -4.0625,  ..., -43.5000, -43.5000, -43.5000],\n",
      "        [-38.0000,   1.6719,  -2.7188,  ..., -37.7500, -38.0000, -37.7500]],\n",
      "       dtype=torch.bfloat16), tensor([[-35.2500,   0.1602,  -2.2500,  ..., -35.0000, -35.2500, -35.0000],\n",
      "        [-38.0000,   0.2656,  -2.1406,  ..., -38.0000, -38.0000, -37.7500]],\n",
      "       dtype=torch.bfloat16), tensor([[-46.7500,  -2.7969,  -4.4688,  ..., -46.5000, -46.5000, -46.2500],\n",
      "        [-40.2500,  -2.2969,  -2.4688,  ..., -40.2500, -40.2500, -40.0000]],\n",
      "       dtype=torch.bfloat16), tensor([[-59.7500,  -6.6250,  -8.1250,  ..., -59.7500, -59.7500, -59.5000],\n",
      "        [-43.0000,  -2.4062,  -4.4062,  ..., -43.0000, -43.0000, -42.7500]],\n",
      "       dtype=torch.bfloat16), tensor([[-60.7500,  -9.5625,  -1.4297,  ..., -60.7500, -60.7500, -60.5000],\n",
      "        [-37.7500,   0.2500,  -1.2656,  ..., -37.7500, -37.7500, -37.5000]],\n",
      "       dtype=torch.bfloat16), tensor([[-61.0000,  -9.4375,  -5.2812,  ..., -61.0000, -61.0000, -60.7500],\n",
      "        [-36.2500,   0.5039,  -1.7734,  ..., -36.2500, -36.5000, -36.0000]],\n",
      "       dtype=torch.bfloat16), tensor([[-56.2500,  -6.4375,   0.4023,  ..., -56.2500, -56.2500, -55.7500],\n",
      "        [-29.7500,   2.1562,   0.2012,  ..., -29.7500, -29.7500, -29.5000]],\n",
      "       dtype=torch.bfloat16), tensor([[-56.7500,  -2.7969,  -6.5000,  ..., -56.7500, -56.7500, -56.5000],\n",
      "        [-39.2500,  -4.0938,  -2.0000,  ..., -39.2500, -39.2500, -39.2500]],\n",
      "       dtype=torch.bfloat16), tensor([[-41.7500,  -0.9258,  -2.4062,  ..., -41.7500, -41.7500, -41.5000],\n",
      "        [-46.2500,  -4.5000,  -4.6250,  ..., -46.2500, -46.2500, -46.0000]],\n",
      "       dtype=torch.bfloat16), tensor([[-52.2500,  -4.8438,  -5.4375,  ..., -52.2500, -52.0000, -52.0000],\n",
      "        [-46.5000,  -1.7422,  -3.7031,  ..., -46.5000, -46.7500, -46.2500]],\n",
      "       dtype=torch.bfloat16), tensor([[-57.2500,  -3.3281,  -8.2500,  ..., -57.2500, -57.0000, -57.0000],\n",
      "        [-41.2500,  -2.8906,   2.8125,  ..., -41.2500, -41.2500, -41.0000]],\n",
      "       dtype=torch.bfloat16), tensor([[-57.2500,   2.6719,  -8.7500,  ..., -57.2500, -57.2500, -57.2500],\n",
      "        [-49.5000,  -1.1484,  -3.0312,  ..., -49.5000, -49.5000, -49.2500]],\n",
      "       dtype=torch.bfloat16), tensor([[-47.0000,   3.6250,  -4.1562,  ..., -47.0000, -47.0000, -47.0000],\n",
      "        [-42.5000,   0.6953,  -3.7500,  ..., -42.5000, -42.5000, -42.2500]],\n",
      "       dtype=torch.bfloat16), tensor([[-36.2500,   2.7031,  -4.2500,  ..., -36.0000, -36.2500, -36.0000],\n",
      "        [-57.5000,   3.0938,  -9.1875,  ..., -57.5000, -57.5000, -57.5000]],\n",
      "       dtype=torch.bfloat16), tensor([[-42.7500,   1.3594,  -4.0625,  ..., -42.7500, -42.7500, -42.5000],\n",
      "        [-35.7500,   4.8438,   5.2188,  ..., -35.7500, -35.7500, -35.5000]],\n",
      "       dtype=torch.bfloat16)]\n",
      "Start logits: tensor([[-57.2500,  -3.3281,  -8.2500,  ..., -57.2500, -57.0000, -57.0000],\n",
      "        [-37.7500,   0.2500,  -1.2656,  ..., -37.7500, -37.7500, -37.5000]],\n",
      "       dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Best logits:\", best_logits)\n",
    "print(\"Start logits:\", start_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
