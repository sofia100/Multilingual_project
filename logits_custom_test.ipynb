{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3,5,7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES= 3,5,7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/home/sofia/cache_custom'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IndicTrans2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from IndicTransToolkit import IndicProcessor\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16 # edited from 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "quantization = None\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.44.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import possible_indic_relations as poss_indic_rel\n",
    "import span_encodings as sp_enc\n",
    "# Reload the module to reflect changes\n",
    "importlib.reload(poss_indic_rel)\n",
    "importlib.reload(sp_enc)\n",
    "\n",
    "pir= poss_indic_rel.possible_relations\n",
    "pir\n",
    "\n",
    "ambiguos_words = list(pir.keys())\n",
    "span_encodings = sp_enc.span_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ଜେଜେମା'\": [41445, 241, 30],\n",
       " 'ଜେଜେବାପା ': [41445, 1007, 1714],\n",
       " 'ମାମୁଁ। ': [9971, 19212, 6],\n",
       " 'ମାଉସୀ। ': [30261, 694, 6],\n",
       " 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252],\n",
       " 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6],\n",
       " 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6],\n",
       " 'ଶିଶୁ ': [3442],\n",
       " 'ପୁତୁରା ': [4300, 5686],\n",
       " 'ଭାଣିଜୀ ': [980, 9742, 795],\n",
       " 'None': [2]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_encodings['ory_Orya']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_and_tokenizer(ckpt_dir, quantization):\n",
    "    if quantization == \"4-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    elif quantization == \"8-bit\":\n",
    "        qconfig = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_use_double_quant=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "    else:\n",
    "        qconfig = None\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        ckpt_dir,\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        quantization_config=qconfig,\n",
    "    )\n",
    "\n",
    "    if qconfig == None:\n",
    "        model = model.to(DEVICE)\n",
    "        if DEVICE == \"cuda\":\n",
    "            model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # ai4bharat/indictrans2-en-indic-dist-200M\n",
    "en_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir,  quantization)\n",
    "\n",
    "ip_en_ind = IndicProcessor(inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resolve_logits_for_best_beam(outputs, num_beams):\n",
    "    \"\"\" Resolve the logits from the best beam, using model output from a generate call.\n",
    "        For a shape [tokens?, batch_size*num_beams, vocab], returns [tokens?, batch_size, vocab]\n",
    "\n",
    "        Assumes num_return_sequences=1.\"\"\"\n",
    "\n",
    "    # print(\"length of output beam\", len(outputs.beam_indices))\n",
    "    # print(\"shape of beam_indices\", outputs.beam_indices.shape)\n",
    "    # print(\"shape of logits\", (outputs.logits[0].shape))\n",
    "    # print(\"length of logits\", len(outputs.logits))\n",
    "    # print(\"length od outputs\", len(outputs))\n",
    "    best_logits  = []\n",
    "    beam_indices = [ outputs.beam_indices[:,i].tolist() for i in range(len(outputs.logits)-1) ]\n",
    "    # print(\"length of beam_indices\", len(beam_indices))\n",
    "\n",
    "    for beam_index, logits in zip(beam_indices, outputs.logits):\n",
    "        beam_index = [ idx if idx != -1 else ((num_beams*(i+1))-1) for i, idx in enumerate(beam_index) ]\n",
    "        best_logits.append(logits[beam_index,:])\n",
    "\n",
    "    return best_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_logits_for_span(logits, translations, tokenizer, search_spans, span_encodings, lang):\n",
    "    \"\"\" Given search spans, returns the logits before the span was generated.\n",
    "\n",
    "    Args:\n",
    "        logits (tuple[Tensor]): Tuple of tensors, of shape [tokens?, batch_size, vocab]\n",
    "        sequences (tuple[list[int]]): Tokenized output sequences.\n",
    "        tokenizer (PreTrainedTokenizerBase): Tokenizer for the model.\n",
    "        search_spans (list[str]): batch_size spans to search for. Must be present in the generated sequences.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Tensor of shape [batch_size, vocab] indicating the logits before the span for each batch element.\n",
    "    \"\"\"\n",
    "    if isinstance(search_spans, str):\n",
    "        search_spans = [ search_spans ] * len(translations)\n",
    "\n",
    "    # with tokenizer.as_target_tokenizer():   \n",
    "    #     detok_outputs = tokenizer.batch_decode(translations, skip_special_tokens=True)\n",
    "\n",
    "    # positions = [ output.index(span) for output, span in zip(translations, search_spans) ]\n",
    "    logit_pos = [  ]\n",
    "\n",
    "    for seq,  span,  in zip(translations,  search_spans): \n",
    "        print(\"Span to search for:\", span, span_encodings[lang])\n",
    "        key = next((k for k in span_encodings[lang].keys() if span in k), \"None\")\n",
    "        subtokens = span_encodings[lang][key]\n",
    "        print(\"subtokens\", subtokens)\n",
    "        print(\"for span:\", span, \"subtokens\", subtokens)\n",
    "        print(\"seq\", seq)\n",
    "        idx = 0\n",
    "        while idx < len(seq):\n",
    "            if any(seq[idx+i] == tok for i, tok in enumerate(subtokens)): \n",
    "                print( \"found\", idx)\n",
    "                break\n",
    "            idx += 1\n",
    "        logit_pos.append(idx-1)\n",
    "    \n",
    "    print(\"logit_pos\", logit_pos)\n",
    "    print(\"Dimensions of logits\", logits[0].shape, len(logits))\n",
    "\n",
    "    selected_logits = []\n",
    "    # Iterate over each batch and corresponding token position\n",
    "    for batch, token in enumerate(logit_pos):\n",
    "        # print(\"batch, token\", batch, token)\n",
    "        # print(\"len(logits)\", len(logits))\n",
    "        # print(\"Shape of logits\", logits[0].shape)\n",
    "        # # print(\"logits token\", logits[token])\n",
    "        # print(\"logits 1st item 1st row\",logits[0][0])\n",
    "        # Extract logits for the specific token position in the current batch\n",
    "        current_logit = logits[token][batch, :]\n",
    "        # current_logit = logits[token][0]\n",
    "        \n",
    "        # Append the selected logit to the list\n",
    "        selected_logits.append(current_logit)\n",
    "\n",
    "    # Stack the list of selected logits into a tensor\n",
    "    selected_logits = torch.stack(selected_logits)\n",
    "\n",
    "    return selected_logits\n",
    "        \n",
    "    # return selected_logits\n",
    "    # return torch.stack([ logits[token][batch,:] for batch, token in enumerate(logit_pos) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_spans(inp_sents, tgt_lang, translations):\n",
    "    search_spans= []\n",
    "    for idx, inp in enumerate(inp_sents):\n",
    "        # check which word from ambiguos_Wwords in present in the input sentence\n",
    "        for word in ambiguos_words:\n",
    "            if word in inp:\n",
    "                curr_amb_word= word\n",
    "\n",
    "        # get the possible relations for the current ambiguous word\n",
    "        possible_relations= pir[curr_amb_word][tgt_lang].keys()\n",
    "        # print(\"Possible relations for the word\", curr_amb_word, \"in\", tgt_lang, \"are\", possible_relations)\n",
    "\n",
    "        # find the word in the translation from the possible relations. if not found print the translation\n",
    "        for rel in possible_relations:\n",
    "            if rel in translations[idx]:\n",
    "                # print(\"Relation found in the translation for the word\", curr_amb_word, \"in\", tgt_lang, \"in the sentence\", translations[idx], \"at \", translations[idx].index(rel), \"\\nso sentece is\", translations[idx][translations[idx].index(rel):])\n",
    "                search_spans.append(rel)\n",
    "                break\n",
    "        else:\n",
    "            print(\"No relation found in the translation for the word\", curr_amb_word, \"in\", tgt_lang, \"in the sentence\", translations[idx])\n",
    "            search_spans.append(translations[idx][0])\n",
    "    print(\"Search spans are\", search_spans)\n",
    "    return search_spans\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip, span_encodings):\n",
    "    translations = []\n",
    "    start_logits = []\n",
    "    for i in tqdm(range(0, len(input_sentences), BATCH_SIZE)):\n",
    "        batch = input_sentences[i : i + BATCH_SIZE]\n",
    "        # Preprocess the batch and extract entity mappings\n",
    "        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "        # Tokenize the batch and generate input encodings\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            # generated_tokens = model.generate(\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1, # TODO temp\n",
    "                output_scores=True,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "\n",
    "            )\n",
    "            print(\"Length of outputs.logits actual\", len(outputs.logits))\n",
    "            print(\"Shape of outputs.logits actual\", outputs.logits[0].shape)\n",
    "\n",
    "            print(\"Length of outputs.beam_indices actual\", len(outputs.beam_indices))\n",
    "            print(\"Shape of outputs.beam_indices actual\", outputs.beam_indices.shape)\n",
    "            \n",
    "            outputs.beam_indices = outputs.beam_indices.cpu()\n",
    "            outputs.logits = tuple(logits.cpu() for logits in outputs.logits)               \n",
    "        # Decode the generated tokens into text\n",
    "        generated_tokens = outputs.sequences\n",
    "        # print(\"len generated_tokens: \", (generated_tokens[0]).shape)\n",
    "        print(\"1st generated token: \", generated_tokens[0])\n",
    "        vector = generated_tokens.detach().cpu().tolist()\n",
    "        # print(\"length of outputs vectors: \", len(vector), len(vector[0]))\n",
    "        # print(\"vector of generated_tokens: \", vector)\n",
    "        print(\"1st vector: \", vector[0])\n",
    "\n",
    "\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            decoded_op = tokenizer.batch_decode(\n",
    "                vector,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "\n",
    "        print(\"1st decoded_op: \", decoded_op[0])\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        translations += ip.postprocess_batch(decoded_op, lang=tgt_lang)\n",
    "\n",
    "        print(\"translations: \", translations)\n",
    "        \n",
    "\n",
    "        search_spans = get_search_spans(batch,  tgt_lang, translations)\n",
    "        print(\"length search_spans: \", len(search_spans))\n",
    "        best_logits = resolve_logits_for_best_beam(outputs, num_beams=5)\n",
    "        # print(\"length best_logits: \",len(best_logits))\n",
    "        print(\"length of best_logits: \", len(best_logits), (best_logits[0]).shape)\n",
    "\n",
    "        start_logits += get_logits_for_span(best_logits, outputs.sequences, tokenizer, search_spans, span_encodings, tgt_lang)\n",
    "        # start_logits += get_logits_for_span(best_logits, translations, tokenizer, search_spans)\n",
    "\n",
    "        del inputs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return translations, start_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_script_list = [\n",
    "                           'ory_Orya',\n",
    "                     'pan_Guru', 'ben_Beng', \n",
    "                       'mal_Mlym',\n",
    "                           'mar_Deva', \n",
    "                           'tam_Taml', 'guj_Gujr', \n",
    "                           'tel_Telu', 'hin_Deva', \n",
    "                           'kan_Knda', \n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_folder = 'custom_test_csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_code_map = {\n",
    "    'eng_Latn': 'en',\n",
    "    'hin_Deva': 'hi',\n",
    "    'guj_Gujr': 'gu',\n",
    "    'kan_Knda': 'kn',\n",
    "    'mal_Mlym': 'ml',\n",
    "    'mar_Deva': 'mr',\n",
    "    'tam_Taml': 'ta',\n",
    "    'tel_Telu': 'te',\n",
    "    'pan_Guru': 'pa',\n",
    "    'ben_Beng': 'bn',\n",
    "    'ory_Orya': 'or'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1809\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "with open('test_sentences_eng.txt', 'r') as f:\n",
    "    sents = f.readlines()\n",
    "sents = [sent.strip() for sent in sents if sent.find('child')==-1]\n",
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My grandmother is a Brahmin.',\n",
       " 'My grandfather is a Brahmin.',\n",
       " 'My uncle is a Brahmin.',\n",
       " 'My aunt is a Brahmin.',\n",
       " 'My brother-in-law is a Brahmin.',\n",
       " 'My sister-in-law is a Brahmin.',\n",
       " 'My cousin is a Brahmin.',\n",
       " 'My nephew is a Brahmin.',\n",
       " 'My niece is a Brahmin.',\n",
       " 'My grandmother is a Kshatriya.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sents[:10]\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn ory_Orya grandmother', 'eng_Latn ory_Orya grandfather', 'eng_Latn ory_Orya uncle', 'eng_Latn ory_Orya aunt', 'eng_Latn ory_Orya brother-in-law', 'eng_Latn ory_Orya sister-in-law', 'eng_Latn ory_Orya cousin', 'eng_Latn ory_Orya child', 'eng_Latn ory_Orya nephew', 'eng_Latn ory_Orya niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sofia/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 41445,   241,    30,     2,     1,     1,     1,     1],\n",
      "       device='cuda:0')\n",
      "1st vector:  [2, 41445, 241, 30, 2, 1, 1, 1, 1]\n",
      "1st decoded_op:  जेजेमा'\n",
      "translations:  [\"ଜେଜେମା'\", 'ଜେଜେବାପା ', 'ମାମୁଁ। ', 'ମାଉସୀ। ', 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ', 'ଶ୍ୱଶୁର। ', 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ', 'ଶିଶୁ ', 'ପୁତୁରା ', 'ଭାଣିଜୀ ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn pan_Guru grandmother', 'eng_Latn pan_Guru grandfather', 'eng_Latn pan_Guru uncle', 'eng_Latn pan_Guru aunt', 'eng_Latn pan_Guru brother-in-law', 'eng_Latn pan_Guru sister-in-law', 'eng_Latn pan_Guru cousin', 'eng_Latn pan_Guru child', 'eng_Latn pan_Guru nephew', 'eng_Latn pan_Guru niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 29498,   640,     2,     1,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 29498, 640, 2, 1, 1, 1]\n",
      "1st decoded_op:  दादी मां \n",
      "translations:  ['ਦਾਦੀ ਮਾਂ ', 'ਦਾਦਾ ਜੀ ', 'ਚਾਚਾ ', 'ਮਾਸੀ ਜੀ। ', 'ਭਰਾ-ਸੱਸ ', 'ਭਰਜਾਈ ', 'ਚਚੇਰਾ ਭਰਾ ', 'ਬੱਚਾ ', 'ਭਤੀਜੇ ', 'ਭਤੀਜੀ ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn ben_Beng grandmother', 'eng_Latn ben_Beng grandfather', 'eng_Latn ben_Beng uncle', 'eng_Latn ben_Beng aunt', 'eng_Latn ben_Beng brother-in-law', 'eng_Latn ben_Beng sister-in-law', 'eng_Latn ben_Beng cousin', 'eng_Latn ben_Beng child', 'eng_Latn ben_Beng nephew', 'eng_Latn ben_Beng niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 48446,   241,     2,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 48446, 241, 2, 1, 1]\n",
      "1st decoded_op:  दिदिमा \n",
      "translations:  ['দিদিমা ', 'দাদা। ', 'চাচা ', 'আন্টি। ', 'শ্যালক ', 'শ্যালিকা ', 'চাচাত ভাই। ', 'শিশু। ', 'ভাগ্নে ', 'ভাগ্নি ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn mal_Mlym grandmother', 'eng_Latn mal_Mlym grandfather', 'eng_Latn mal_Mlym uncle', 'eng_Latn mal_Mlym aunt', 'eng_Latn mal_Mlym brother-in-law', 'eng_Latn mal_Mlym sister-in-law', 'eng_Latn mal_Mlym cousin', 'eng_Latn mal_Mlym child', 'eng_Latn mal_Mlym nephew', 'eng_Latn mal_Mlym niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 18823,  2914,  2826,     2,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 18823, 2914, 2826, 2, 1, 1]\n",
      "1st decoded_op:  मुत्तश्शि \n",
      "translations:  ['മുത്തശ്ശി ', 'മുത്തച്ഛൻ ', 'അമ്മാവൻ ', 'അമ്മായി. ', 'ഭാര്യാസഹോദരൻ ', 'ഭാര്യാസഹോദരി ', 'കസിൻ ', 'കുട്ടി. ', 'അനന്തരവൻ ', 'അനന്തരവൾ ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn mar_Deva grandmother', 'eng_Latn mar_Deva grandfather', 'eng_Latn mar_Deva uncle', 'eng_Latn mar_Deva aunt', 'eng_Latn mar_Deva brother-in-law', 'eng_Latn mar_Deva sister-in-law', 'eng_Latn mar_Deva cousin', 'eng_Latn mar_Deva child', 'eng_Latn mar_Deva nephew', 'eng_Latn mar_Deva niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 32967,     2,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 32967, 2, 1, 1]\n",
      "1st decoded_op:  आजी \n",
      "translations:  ['आजी ', 'आजोबा. ', 'काका. ', 'मावशी. ', 'मेहुणा ', 'मेहुणी ', 'चुलत भाऊ ', 'बाळ. ', 'पुतण्या ', 'भाची ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn tam_Taml grandmother', 'eng_Latn tam_Taml grandfather', 'eng_Latn tam_Taml uncle', 'eng_Latn tam_Taml aunt', 'eng_Latn tam_Taml brother-in-law', 'eng_Latn tam_Taml sister-in-law', 'eng_Latn tam_Taml cousin', 'eng_Latn tam_Taml child', 'eng_Latn tam_Taml nephew', 'eng_Latn tam_Taml niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([  2, 511, 956,   2,   1], device='cuda:0')\n",
      "1st vector:  [2, 511, 956, 2, 1]\n",
      "1st decoded_op:  पाट्टि \n",
      "translations:  ['பாட்டி ', 'தாத்தா. ', 'மாமா ', 'அத்தை ', 'மைத்துனர் ', 'மைத்துனர் ', 'உறவினர். ', 'குழந்தை. ', 'மருமகன் ', 'மருமகள் ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn guj_Gujr grandmother', 'eng_Latn guj_Gujr grandfather', 'eng_Latn guj_Gujr uncle', 'eng_Latn guj_Gujr aunt', 'eng_Latn guj_Gujr brother-in-law', 'eng_Latn guj_Gujr sister-in-law', 'eng_Latn guj_Gujr cousin', 'eng_Latn guj_Gujr child', 'eng_Latn guj_Gujr nephew', 'eng_Latn guj_Gujr niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 29498,     2,     1,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 29498, 2, 1, 1, 1]\n",
      "1st decoded_op:  दादी \n",
      "translations:  ['દાદી ', 'દાદા ', 'કાકા ', 'કાકી ', 'સાળા ', 'સાસુ-સસરા ', 'પિતરાઇ ભાઇ ', 'બાળક ', 'ભત્રીજો ', 'ભત્રીજી ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn tel_Telu grandmother', 'eng_Latn tel_Telu grandfather', 'eng_Latn tel_Telu uncle', 'eng_Latn tel_Telu aunt', 'eng_Latn tel_Telu brother-in-law', 'eng_Latn tel_Telu sister-in-law', 'eng_Latn tel_Telu cousin', 'eng_Latn tel_Telu child', 'eng_Latn tel_Telu nephew', 'eng_Latn tel_Telu niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([   2, 1774, 1476,    2,    1,    1], device='cuda:0')\n",
      "1st vector:  [2, 1774, 1476, 2, 1, 1]\n",
      "1st decoded_op:  अम्मम्म \n",
      "translations:  ['అమ్మమ్మ ', 'తాతయ్య ', 'అంకుల్ ', 'అత్తగారు ', 'బావమరిది ', 'చెల్లెలు ', 'బంధువు ', 'పిల్లవాడు. ', 'మేనల్లుడు ', 'మేనకోడలు ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn hin_Deva grandmother', 'eng_Latn hin_Deva grandfather', 'eng_Latn hin_Deva uncle', 'eng_Latn hin_Deva aunt', 'eng_Latn hin_Deva brother-in-law', 'eng_Latn hin_Deva sister-in-law', 'eng_Latn hin_Deva cousin', 'eng_Latn hin_Deva child', 'eng_Latn hin_Deva nephew', 'eng_Latn hin_Deva niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2, 29498, 12075,     2,     1], device='cuda:0')\n",
      "1st vector:  [2, 29498, 12075, 2, 1]\n",
      "1st decoded_op:  दादी माँ \n",
      "translations:  ['दादी माँ ', 'दादा जी। ', 'चाचा ', 'चाची ', 'बहनोई ', 'ननद ', 'चचेरा भाई ', 'बच्चा। ', 'भतीजे ', 'भतीजी ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: ['grandmother', 'grandfather', 'uncle', 'aunt', 'brother-in-law', 'sister-in-law', 'cousin', 'child', 'nephew', 'niece']\n",
      "Batch: ['eng_Latn kan_Knda grandmother', 'eng_Latn kan_Knda grandfather', 'eng_Latn kan_Knda uncle', 'eng_Latn kan_Knda aunt', 'eng_Latn kan_Knda brother-in-law', 'eng_Latn kan_Knda sister-in-law', 'eng_Latn kan_Knda cousin', 'eng_Latn kan_Knda child', 'eng_Latn kan_Knda nephew', 'eng_Latn kan_Knda niece']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st generated token:  tensor([    2,  4565, 35330,     2,     1,     1], device='cuda:0')\n",
      "1st vector:  [2, 4565, 35330, 2, 1, 1]\n",
      "1st decoded_op:  अज्जि \n",
      "translations:  ['ಅಜ್ಜಿ ', 'ಅಜ್ಜ ', 'ಚಿಕ್ಕಪ್ಪ ', 'ಚಿಕ್ಕಮ್ಮ. ', 'ಸೋದರ ಸಂಬಂಧಿ ', 'ಅತ್ತಿಗೆ ', 'ಸೋದರಸಂಬಂಧಿ ', 'ಮಗು. ', 'ಸೋದರಳಿಯ ', 'ಸೋದರ ಸೊಸೆ ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ory_Orya': {\"ଜେଜେମା'\": [41445, 241, 30],\n",
       "  'ଜେଜେବାପା ': [41445, 1007, 1714],\n",
       "  'ମାମୁଁ। ': [9971, 19212, 6],\n",
       "  'ମାଉସୀ। ': [30261, 694, 6],\n",
       "  'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252],\n",
       "  'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6],\n",
       "  'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6],\n",
       "  'ଶିଶୁ ': [3442],\n",
       "  'ପୁତୁରା ': [4300, 5686],\n",
       "  'ଭାଣିଜୀ ': [980, 9742, 795]},\n",
       " 'pan_Guru': {'ਦਾਦੀ ਮਾਂ ': [29498, 640],\n",
       "  'ਦਾਦਾ ਜੀ ': [15588, 613],\n",
       "  'ਚਾਚਾ ': [34059],\n",
       "  'ਮਾਸੀ ਜੀ। ': [65770, 613, 6],\n",
       "  'ਭਰਾ-ਸੱਸ ': [8327, 13, 116, 19, 115],\n",
       "  'ਭਰਜਾਈ ': [1144, 40842],\n",
       "  'ਚਚੇਰਾ ਭਰਾ ': [49615, 2656, 8327],\n",
       "  'ਬੱਚਾ ': [336, 19, 317],\n",
       "  'ਭਤੀਜੇ ': [39136, 1386],\n",
       "  'ਭਤੀਜੀ ': [39136, 795]},\n",
       " 'ben_Beng': {'দিদিমা ': [48446, 241],\n",
       "  'দাদা। ': [15588, 6],\n",
       "  'চাচা ': [34059],\n",
       "  'আন্টি। ': [5745, 102, 6],\n",
       "  'শ্যালক ': [649, 4692, 75],\n",
       "  'শ্যালিকা ': [649, 4692, 1525],\n",
       "  'চাচাত ভাই। ': [35407, 359, 3991, 6],\n",
       "  'শিশু। ': [3442, 6],\n",
       "  'ভাগ্নে ': [291, 27065],\n",
       "  'ভাগ্নি ': [291, 17389]},\n",
       " 'mal_Mlym': {'മുത്തശ്ശി ': [18823, 2914, 2826],\n",
       "  'മുത്തച്ഛൻ ': [18823, 5136, 28],\n",
       "  'അമ്മാവൻ ': [1774, 529, 28],\n",
       "  'അമ്മായി. ': [16343, 4],\n",
       "  'ഭാര്യാസഹോദരൻ ': [3954, 4109, 3426, 877, 28],\n",
       "  'ഭാര്യാസഹോദരി ': [3954, 4109, 3426, 50084],\n",
       "  'കസിൻ ': [43494, 28],\n",
       "  'കുട്ടി. ': [14325, 4],\n",
       "  'അനന്തരവൻ ': [1326, 13315, 128, 28],\n",
       "  'അനന്തരവൾ ': [1326, 13315, 128, 27]},\n",
       " 'mar_Deva': {'आजी ': [32967],\n",
       "  'आजोबा. ': [56799, 4],\n",
       "  'काका. ': [41565, 4],\n",
       "  'मावशी. ': [11701, 987, 4],\n",
       "  'मेहुणा ': [530, 7131, 1067],\n",
       "  'मेहुणी ': [530, 7131, 1730],\n",
       "  'चुलत भाऊ ': [6702, 119, 22699],\n",
       "  'बाळ. ': [6376, 4],\n",
       "  'पुतण्या ': [4300, 12291],\n",
       "  'भाची ': [4569, 362]},\n",
       " 'tam_Taml': {'பாட்டி ': [511, 956],\n",
       "  'தாத்தா. ': [14552, 3286, 4],\n",
       "  'மாமா ': [24501],\n",
       "  'அத்தை ': [213, 681],\n",
       "  'மைத்துனர் ': [1364, 864, 1488],\n",
       "  'உறவினர். ': [2901, 47743, 4],\n",
       "  'குழந்தை. ': [9219, 4],\n",
       "  'மருமகன் ': [45894, 319],\n",
       "  'மருமகள் ': [45894, 1022]},\n",
       " 'guj_Gujr': {'દાદી ': [29498],\n",
       "  'દાદા ': [15588],\n",
       "  'કાકા ': [41565],\n",
       "  'કાકી ': [64851],\n",
       "  'સાળા ': [1872, 1624],\n",
       "  'સાસુ-સસરા ': [63122, 13, 116, 41996],\n",
       "  'પિતરાઇ ભાઇ ': [6042, 18974, 3991],\n",
       "  'બાળક ': [6199],\n",
       "  'ભત્રીજો ': [980, 14910, 3204],\n",
       "  'ભત્રીજી ': [980, 14910, 795]},\n",
       " 'tel_Telu': {'అమ్మమ్మ ': [1774, 1476],\n",
       "  'తాతయ్య ': [14552, 4559],\n",
       "  'అంకుల్ ': [4272, 6246],\n",
       "  'అత్తగారు ': [11569, 20269],\n",
       "  'బావమరిది ': [19360, 143, 399, 468],\n",
       "  'చెల్లెలు ': [1978, 104, 124],\n",
       "  'బంధువు ': [2922, 3432],\n",
       "  'పిల్లవాడు. ': [5382, 12324, 4],\n",
       "  'మేనల్లుడు ': [14581, 471, 4645],\n",
       "  'మేనకోడలు ': [14581, 307, 12563]},\n",
       " 'hin_Deva': {'दादी माँ ': [29498, 12075],\n",
       "  'दादा जी। ': [15588, 613, 6],\n",
       "  'चाचा ': [34059],\n",
       "  'चाची ': [60684],\n",
       "  'बहनोई ': [4615, 49403],\n",
       "  'ननद ': [3544, 64],\n",
       "  'चचेरा भाई ': [49615, 2656, 3057],\n",
       "  'बच्चा। ': [13233, 6],\n",
       "  'भतीजे ': [39136, 1386],\n",
       "  'भतीजी ': [39136, 795]},\n",
       " 'kan_Knda': {'ಅಜ್ಜಿ ': [4565, 35330],\n",
       "  'ಅಜ್ಜ ': [44928],\n",
       "  'ಚಿಕ್ಕಪ್ಪ ': [2950, 378],\n",
       "  'ಚಿಕ್ಕಮ್ಮ. ': [2950, 1476, 4],\n",
       "  'ಸೋದರ ಸಂಬಂಧಿ ': [34385, 35471],\n",
       "  'ಅತ್ತಿಗೆ ': [213, 24430],\n",
       "  'ಸೋದರಸಂಬಂಧಿ ': [34385, 2182, 15841, 2603],\n",
       "  'ಮಗು. ': [15967, 4],\n",
       "  'ಸೋದರಳಿಯ ': [34385, 5999],\n",
       "  'ಸೋದರ ಸೊಸೆ ': [34385, 6324, 3658]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_trl=[]\n",
    "span_encodings = {}\n",
    "for lang in lang_script_list:\n",
    "#   if lang == 'ory_Orya':\n",
    "    span_encodings[lang] = {}\n",
    "    inputs = ambiguos_words\n",
    "    for i in tqdm(range(0, len(inputs), BATCH_SIZE)):\n",
    "        batch = inputs[i : i + BATCH_SIZE]\n",
    "        print(\"Batch:\", batch)  \n",
    "\n",
    "        # batch = ip_en_ind.preprocess_batch(words_ids[lang].keys().tolist(), src_lang=lang, tgt_lang=lang)\n",
    "        batch = ip_en_ind.preprocess_batch(batch, src_lang='eng_Latn', tgt_lang=lang)\n",
    "        print(\"Batch:\", batch)\n",
    "        # # Tokenize the batch and generate input encodings\n",
    "        inputs = en_indic_tokenizer(\n",
    "            batch,\n",
    "            truncation=True,\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # generated_tokens = model.generate(\n",
    "            outputs = en_indic_model.generate(\n",
    "                **inputs,\n",
    "                use_cache=True,\n",
    "                min_length=0,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1, # TODO temp\n",
    "                output_scores=True,\n",
    "                output_logits=True,\n",
    "                return_dict_in_generate=True,\n",
    "\n",
    "            )\n",
    "            # print(\"Length of outputs.logits actual\", len(outputs.logits))\n",
    "            # print(\"Shape of outputs.logits actual\", outputs.logits[0].shape)\n",
    "\n",
    "            # print(\"Length of outputs.beam_indices actual\", len(outputs.beam_indices))\n",
    "            # print(\"Shape of outputs.beam_indices actual\", outputs.beam_indices.shape)\n",
    "            \n",
    "            outputs.beam_indices = outputs.beam_indices.cpu()\n",
    "            outputs.logits = tuple(logits.cpu() for logits in outputs.logits)               \n",
    "        # Decode the generated tokens into text\n",
    "        generated_tokens = outputs.sequences\n",
    "        # print(\"len generated_tokens: \", (generated_tokens[0]).shape)\n",
    "        print(\"1st generated token: \", generated_tokens[0])\n",
    "        vector = generated_tokens.detach().cpu().tolist()\n",
    "        # print(\"length of outputs vectors: \", len(vector), len(vector[0]))\n",
    "        # print(\"vector of generated_tokens: \", vector)\n",
    "        print(\"1st vector: \", vector[0])\n",
    "        \n",
    "\n",
    "\n",
    "        with en_indic_tokenizer.as_target_tokenizer():\n",
    "            decoded_op = en_indic_tokenizer.batch_decode(\n",
    "                vector,\n",
    "                skip_special_tokens=True,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "\n",
    "        print(\"1st decoded_op: \", decoded_op[0])\n",
    "        # Postprocess the translations, including entity replacement\n",
    "        word_trl = ip_en_ind.postprocess_batch(decoded_op, lang=lang)\n",
    "\n",
    "        print(\"translations: \", word_trl)\n",
    "        for word in word_trl:\n",
    "            word_index = word_trl.index(word)\n",
    "            if word_index < len(vector):\n",
    "                span_encodings[lang][word] = vector[word_index]\n",
    "                # keep the items between '2' and '2' from span_encodings[lang][word]\n",
    "                start_idx = vector[word_index].index(2)\n",
    "                end_idx = vector[word_index].index(2, start_idx+1)\n",
    "                span_encodings[lang][word] = vector[word_index][start_idx+1:end_idx]\n",
    "            else:\n",
    "                print(f\"Index {word_index} out of range for vector of length {len(vector)}\")\n",
    "\n",
    "\n",
    "span_encodings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ory_Orya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of outputs.logits actual 10\n",
      "Shape of outputs.logits actual torch.Size([50, 122672])\n",
      "Length of outputs.beam_indices actual 10\n",
      "Shape of outputs.beam_indices actual torch.Size([10, 9])\n",
      "1st generated token:  tensor([    2,   644, 41445,   241,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "1st vector:  [2, 644, 41445, 241, 4725, 28125, 6, 2, 1]\n",
      "1st decoded_op:  मो जेजेमा जणे ब्राह्मण । \n",
      "translations:  ['ମୋ ଜେଜେମା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଜେଜେବାପା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ମାମୁଁ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ମାଉସୀ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଭିଣୋଇ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଭାଉଜ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋର ସମ୍ପର୍କୀଯ଼ ଭାଇ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ପୁତୁରା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋର ଭାଣିଜୀ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଜେଜେମା ଜଣେ କ୍ଷତ୍ରିଯ଼। ']\n",
      "Search spans are ['ଜେଜେମା', 'ଜେଜେବାପା', 'ମାମୁଁ', 'ମାଉସୀ', 'ଭିଣୋଇ', 'ଭାଉଜ', 'ଭାଇ', 'ପୁତୁରା', 'ଭାଣିଜୀ', 'ଜେଜେମା']\n",
      "length search_spans:  10\n",
      "length of best_logits:  9 torch.Size([10, 122672])\n",
      "Span to search for: ଜେଜେମା {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [41445, 241, 30]\n",
      "for span: ଜେଜେମା subtokens [41445, 241, 30]\n",
      "seq tensor([    2,   644, 41445,   241,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ଜେଜେବାପା {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [41445, 1007, 1714]\n",
      "for span: ଜେଜେବାପା subtokens [41445, 1007, 1714]\n",
      "seq tensor([    2,   644, 41445,  1007,  1714,  4725, 28125,     6,     2],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ମାମୁଁ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [9971, 19212, 6]\n",
      "for span: ମାମୁଁ subtokens [9971, 19212, 6]\n",
      "seq tensor([    2,   644,  9971, 19212,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ମାଉସୀ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [30261, 694, 6]\n",
      "for span: ମାଉସୀ subtokens [30261, 694, 6]\n",
      "seq tensor([    2,   644, 30261,   694,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ଭିଣୋଇ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [2]\n",
      "for span: ଭିଣୋଇ subtokens [2]\n",
      "seq tensor([    2,   644,  5442,  1754,    89,  4725, 28125,     6,     2],\n",
      "       device='cuda:0')\n",
      "found 0\n",
      "Span to search for: ଭାଉଜ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [2]\n",
      "for span: ଭାଉଜ subtokens [2]\n",
      "seq tensor([    2,   644,  4569, 25547,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 0\n",
      "Span to search for: ଭାଇ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [60824, 3991, 6]\n",
      "for span: ଭାଇ subtokens [60824, 3991, 6]\n",
      "seq tensor([    2,  4433, 60824,  3991,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ପୁତୁରା {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [4300, 5686]\n",
      "for span: ପୁତୁରା subtokens [4300, 5686]\n",
      "seq tensor([    2,   644,  4300,  5686,  4725, 28125,     6,     2,     1],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ଭାଣିଜୀ {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [980, 9742, 795]\n",
      "for span: ଭାଣିଜୀ subtokens [980, 9742, 795]\n",
      "seq tensor([    2,  4433,   980,  9742,   795,  4725, 28125,     6,     2],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "Span to search for: ଜେଜେମା {\"ଜେଜେମା'\": [41445, 241, 30], 'ଜେଜେବାପା ': [41445, 1007, 1714], 'ମାମୁଁ। ': [9971, 19212, 6], 'ମାଉସୀ। ': [30261, 694, 6], 'ଶ୍ୱଶୁର-ଶ୍ୱଶୁର ': [21405, 699, 22252, 13, 21405, 699, 22252], 'ଶ୍ୱଶୁର। ': [21405, 699, 22252, 6], 'ସମ୍ପର୍କୀଯ଼ ଭାଇ। ': [60824, 3991, 6], 'ଶିଶୁ ': [3442], 'ପୁତୁରା ': [4300, 5686], 'ଭାଣିଜୀ ': [980, 9742, 795], 'None': [2]}\n",
      "subtokens [41445, 241, 30]\n",
      "for span: ଜେଜେମା subtokens [41445, 241, 30]\n",
      "seq tensor([    2,   644, 41445,   241,  4725, 65121,  2383,     6,     2],\n",
      "       device='cuda:0')\n",
      "found 2\n",
      "logit_pos [1, 1, 1, 1, -1, -1, 1, 1, 1, 1]\n",
      "Dimensions of logits torch.Size([10, 122672]) 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test_trasnlations/indic_trans2/test_transl_it2_ory_Orya_logits.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m translations, logits \u001b[38;5;241m=\u001b[39m batch_translate(sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip_en_ind, span_encodings)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# # save hindi translations to a file test_translations_hin.txt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# with open('test_translations/indic_trans2/test_transl_it2_'+lang+'.txt', 'w') as f:\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     for sent in translations:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         f.write(sent + '\\n')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# save the logits to a file \u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_trasnlations/indic_trans2/test_transl_it2_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_logits.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m logit \u001b[38;5;129;01min\u001b[39;00m logits:\n\u001b[1;32m     17\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(logit) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test_trasnlations/indic_trans2/test_transl_it2_ory_Orya_logits.txt'"
     ]
    }
   ],
   "source": [
    "src_lang = \"eng_Latn\"\n",
    "\n",
    "for lang in lang_script_list:\n",
    "    tgt_lang = lang\n",
    "    print(lang)\n",
    "    translations, logits = batch_translate(sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip_en_ind, span_encodings)\n",
    "\n",
    "\n",
    "    # # save hindi translations to a file test_translations_hin.txt\n",
    "    # with open('test_translations/indic_trans2/test_transl_it2_'+lang+'.txt', 'w') as f:\n",
    "    #     for sent in translations:\n",
    "    #         f.write(sent + '\\n')\n",
    "\n",
    "    # save the logits to a file \n",
    "    # with open('test_trasnlations/indic_trans2/test_transl_it2_'+lang+'_logits.txt', 'w') as f:\n",
    "    #     for logit in logits:\n",
    "    #         f.write(str(logit) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import the test_sentences_eng.txt file data as sents\n",
    "\n",
    "# # sents = ['The river is blue.', 'My uncle wearing blue coat.', 'My maternal aunt went to the river bank.', \"The doctor went to bank for money.\"]\n",
    "# sents=['My maternal aunt went to the river bank.', 'My maternal aunt loves me.', \n",
    "#        'My maternal aunt loves her grandchildren.', \"My maternal aunt loves her children.\", \n",
    "#        \"My maternal aunt loves her family.\",\n",
    "#        ]   \n",
    "# src_lang = \"eng_Latn\"\n",
    "\n",
    "# tgt_lang = 'hin_Deva'\n",
    "# # print(lang)\n",
    "# translations, logits = batch_translate(sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip_en_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ମୋ ଜେଜେମା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଜେଜେବାପା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ମାମୁଁ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ମାଉସୀ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଭିଣୋଇ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଭାଉଜ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋର ସମ୍ପର୍କୀଯ଼ ଭାଇ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ପୁତୁରା ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋର ଭାଣିଜୀ ଜଣେ ବ୍ରାହ୍ମଣ। ', 'ମୋ ଜେଜେମା ଜଣେ କ୍ଷତ୍ରିଯ଼। ']\n"
     ]
    }
   ],
   "source": [
    "print(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-1.5371, -1.5361, -2.9688,  ..., -1.5361, -1.5361, -1.5371],\n",
      "       dtype=torch.float16), tensor([-1.3369, -1.3369, -2.5410,  ..., -1.3369, -1.3369, -1.3369],\n",
      "       dtype=torch.float16), tensor([-1.5029, -1.5029, -1.8184,  ..., -1.5029, -1.5029, -1.5029],\n",
      "       dtype=torch.float16), tensor([-1.9922, -1.9912, -2.8516,  ..., -1.9912, -1.9912, -1.9912],\n",
      "       dtype=torch.float16), tensor([-0.1608, -0.1608, 13.7422,  ..., -0.1608, -0.1608, -0.1608],\n",
      "       dtype=torch.float16), tensor([ 0.0404,  0.0405, 13.9375,  ...,  0.0404,  0.0405,  0.0405],\n",
      "       dtype=torch.float16), tensor([-1.3818, -1.3818, -2.6504,  ..., -1.3818, -1.3818, -1.3818],\n",
      "       dtype=torch.float16), tensor([-1.5996, -1.5996, -2.4492,  ..., -1.5996, -1.5996, -1.5996],\n",
      "       dtype=torch.float16), tensor([-1.6621, -1.6611, -3.1562,  ..., -1.6611, -1.6611, -1.6611],\n",
      "       dtype=torch.float16), tensor([-1.4980, -1.4971, -2.8984,  ..., -1.4971, -1.4971, -1.4980],\n",
      "       dtype=torch.float16)]\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to break all run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3.5763e-07, 3.5763e-07, 5.9605e-08,  ..., 3.5763e-07, 3.5763e-07,\n",
      "        3.5763e-07], dtype=torch.float16), tensor([4.1723e-07, 4.1723e-07, 1.1921e-07,  ..., 4.1723e-07, 4.1723e-07,\n",
      "        4.1723e-07], dtype=torch.float16), tensor([9.5367e-07, 9.5367e-07, 6.5565e-07,  ..., 9.5367e-07, 9.5367e-07,\n",
      "        9.5367e-07], dtype=torch.float16), tensor([1.0133e-06, 1.0133e-06, 4.1723e-07,  ..., 1.0133e-06, 1.0133e-06,\n",
      "        1.0133e-06], dtype=torch.float16), tensor([8.3447e-07, 8.3447e-07, 8.9697e-01,  ..., 8.3447e-07, 8.3447e-07,\n",
      "        8.3447e-07], dtype=torch.float16), tensor([8.3447e-07, 8.3447e-07, 8.9795e-01,  ..., 8.3447e-07, 8.3447e-07,\n",
      "        8.3447e-07], dtype=torch.float16), tensor([1.0133e-06, 1.0133e-06, 2.9802e-07,  ..., 1.0133e-06, 1.0133e-06,\n",
      "        1.0133e-06], dtype=torch.float16), tensor([3.5763e-07, 3.5763e-07, 1.7881e-07,  ..., 3.5763e-07, 3.5763e-07,\n",
      "        3.5763e-07], dtype=torch.float16), tensor([1.0133e-06, 1.0133e-06, 2.3842e-07,  ..., 1.0133e-06, 1.0133e-06,\n",
      "        1.0133e-06], dtype=torch.float16), tensor([3.5763e-07, 3.5763e-07, 5.9605e-08,  ..., 3.5763e-07, 3.5763e-07,\n",
      "        3.5763e-07], dtype=torch.float16)]\n"
     ]
    }
   ],
   "source": [
    "# for each logit in logits, convert it to prob using softmax\n",
    "\n",
    "softmax_logits = [softmax(logit, dim=-1) for logit in logits]\n",
    "print(softmax_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(41445), tensor(41445), tensor(9971), tensor(30261), tensor(2), tensor(2), tensor(60824), tensor(4300), tensor(980), tensor(41445)]\n"
     ]
    }
   ],
   "source": [
    "# get the index of max prob for each logit\n",
    "max_prob_index = [torch.argmax(softmax_logit, dim=-1) for softmax_logit in softmax_logits]\n",
    "print(max_prob_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.return_types.topk(\n",
      "values=tensor([0.8813, 0.0048, 0.0036, 0.0031, 0.0028, 0.0021, 0.0021, 0.0016, 0.0016,\n",
      "        0.0014], dtype=torch.float16),\n",
      "indices=tensor([41445,    80,    30,  8911, 51174, 29498, 33622, 48446, 30261, 30502])), torch.return_types.topk(\n",
      "values=tensor([0.8306, 0.0312, 0.0203, 0.0101, 0.0086, 0.0077, 0.0056, 0.0024, 0.0017,\n",
      "        0.0010], dtype=torch.float16),\n",
      "indices=tensor([41445, 53208, 15588, 20077, 30502,    30,   409,  1111, 55513, 51174])), torch.return_types.topk(\n",
      "values=tensor([0.5098, 0.1919, 0.0316, 0.0116, 0.0104, 0.0051, 0.0042, 0.0039, 0.0035,\n",
      "        0.0034], dtype=torch.float16),\n",
      "indices=tensor([ 9971, 41565, 24501,    30, 41445,  7610, 20077,  3991,  3617, 15588])), torch.return_types.topk(\n",
      "values=tensor([0.6040, 0.0272, 0.0157, 0.0095, 0.0076, 0.0057, 0.0053, 0.0051, 0.0051,\n",
      "        0.0049], dtype=torch.float16),\n",
      "indices=tensor([30261, 41445,  9971,    30,  3617,   213, 54203,   336, 58401,  1390])), torch.return_types.topk(\n",
      "values=tensor([8.9697e-01, 3.3832e-04, 1.7762e-04, 1.0604e-04, 4.3333e-05, 2.4915e-05,\n",
      "        1.3947e-05, 7.9274e-06, 7.1526e-06, 7.0333e-06], dtype=torch.float16),\n",
      "indices=tensor([   2,    6,    4,    7,   13,    5, 9182, 2207,   30,   22])), torch.return_types.topk(\n",
      "values=tensor([8.9795e-01, 1.4794e-04, 3.5584e-05, 1.6153e-05, 8.7619e-06, 8.4639e-06,\n",
      "        7.7486e-06, 5.6624e-06, 5.4240e-06, 3.9339e-06], dtype=torch.float16),\n",
      "indices=tensor([   2,    6,    4,    7, 9182,   13,    5,   40, 2207,   11])), torch.return_types.topk(\n",
      "values=tensor([0.5742, 0.0252, 0.0235, 0.0147, 0.0088, 0.0081, 0.0079, 0.0065, 0.0049,\n",
      "        0.0049], dtype=torch.float16),\n",
      "indices=tensor([60824,  3991,  9971, 56796,  6091, 38982,  4725,  1111, 21058,   110])), torch.return_types.topk(\n",
      "values=tensor([0.6226, 0.2020, 0.0210, 0.0135, 0.0099, 0.0072, 0.0047, 0.0047, 0.0032,\n",
      "        0.0029], dtype=torch.float16),\n",
      "indices=tensor([ 4300, 30360,   980,  3991,    30,  1013, 21058,  4569,  9919, 60824])), torch.return_types.topk(\n",
      "values=tensor([0.4551, 0.1151, 0.0444, 0.0205, 0.0179, 0.0153, 0.0098, 0.0091, 0.0066,\n",
      "        0.0057], dtype=torch.float16),\n",
      "indices=tensor([  980, 30360,  4300,  2398,  4569, 38982, 13818,  3991,  1390,  4725])), torch.return_types.topk(\n",
      "values=tensor([0.8970, 0.0046, 0.0034, 0.0029, 0.0023, 0.0020, 0.0015, 0.0014, 0.0014,\n",
      "        0.0013], dtype=torch.float16),\n",
      "indices=tensor([41445,    80,  8911,    30, 29498, 51174, 48446, 33622,  1828,  2214]))]\n"
     ]
    }
   ],
   "source": [
    "# from the softmax_logits, get the indices of top 10 max prob \n",
    "top_10_indices = [torch.topk(softmax_logit, k=10, dim=-1) for softmax_logit in softmax_logits]\n",
    "print(top_10_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['जेजे ', 'न ', \"' \", 'नानी ', 'नाति ', 'दादी ', 'माआ ', 'दिदि ', 'माउ ', 'पिताम '], ['जेजे ', 'पितामह ', 'दादा ', 'बापा ', 'पिताम ', \"' \", 'द ', 'बड़ ', 'बापाङ्क ', 'नाति '], ['माम ', 'काका ', 'मामा ', \"' \", 'जेजे ', 'काक ', 'बापा ', 'भाइ ', 'खु ', 'दादा '], ['माउ ', 'जेजे ', 'माम ', \"' \", 'खु ', 'अ ', 'माङ्क ', 'ब ', 'कुनि ', 'बो '], ['</s> ', '। ', '. ', '\" ', '- ', ', ', '| ', 'I ', \"' \", '( '], ['</s> ', '। ', '. ', '\" ', '| ', '- ', ', ', '! ', 'I ', '? '], ['सम्पर्कीय़ ', 'भाइ ', 'माम ', 'भ्रातृ ', 'सम्प ', 'भउणी ', 'जणे ', 'बड़ ', 'भ्र ', 'क '], ['पुत ', 'भण ', 'भ ', 'भाइ ', \"' \", 'पु ', 'भ्र ', 'भा ', 'सान ', 'सम्पर्कीय़ '], ['भ ', 'भण ', 'पुत ', 'झ ', 'भा ', 'भउणी ', 'भग ', 'भाइ ', 'बो ', 'जणे '], ['जेजे ', 'न ', 'नानी ', \"' \", 'दादी ', 'नाति ', 'दिदि ', 'माआ ', 'आइ ', 'मात ']]\n"
     ]
    }
   ],
   "source": [
    "# from the top_10_indices, get the words in Hindi vocab using tokenizer.target_tokenizer\n",
    "word_list = []\n",
    "for i in top_10_indices:\n",
    "    with en_indic_tokenizer.as_target_tokenizer():\n",
    "        word_list.append(en_indic_tokenizer.batch_decode(i.indices))\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in word_list:\n",
    "    print(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
